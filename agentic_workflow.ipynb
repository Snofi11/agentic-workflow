{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Workflow with HuggingFace InferenceClient + RAG\n",
    "\n",
    "Production-ready workflow with advanced features:\n",
    "- âœ… Using HuggingFace InferenceClient for LLM calls that use `chat.completions.create()` method compatible with all HuggingFace Inference and OpenAI API\n",
    "- âœ… **RAG (Retrieval-Augmented Generation)** with LangChain vector store retriever integration\n",
    "- âœ… Retry policies with exponential backoff, error handling, and workflow logic\n",
    "- âœ… Checkpointing for fault tolerance\n",
    "- âœ… LLM caching for performance\n",
    "- âœ… Structured logging and error handling\n",
    "- âœ… Multi-node workflow (Analyzer â†’ Planner â†’ Executor with RAG â†’ Evaluator)\n",
    "- âœ… Streaming support for real-time updates\n",
    "- âœ… Chroma vector store for document retrieval with MMR search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ RAG-Enhanced AgenticWorkflow\n",
    "\n",
    "\n",
    "**Implementation Summary:**\n",
    "\n",
    "The `AgenticWorkflow` class configured to use HuggingFace InferenceClient with OpenAI-compatible API **and RAG (Retrieval-Augmented Generation)**.\n",
    "\n",
    "**Key features:**\n",
    "\n",
    "âœ… **API Compatibility:**\n",
    "- OpenAI-compatible message format\n",
    "- Compatible with all HuggingFace Inference API models\n",
    "- Multi-provider support (hyperbolic, nebius, together, etc.)\n",
    "- Native streaming capabilities\n",
    "\n",
    "âœ¨ **Fully Integrated LangChain RAG**  \n",
    "- Analyzer â†’ Planner â†’ **Executor (with RAG)** â†’ Evaluator\n",
    "- Document retrieval from Chroma vector store\n",
    "- MMR search for relevant + diverse results\n",
    "- Automatic synthesis of retrieved context with LLM responses\n",
    "\n",
    "ğŸ“š **Knowledge Base**  \n",
    "- Vector store: Chroma (persistent)\n",
    "- Embeddings: all-MiniLM-L6-v2\n",
    "- Content: Transformer paper (Attention Is All You Need)\n",
    "- Search: Top-3 documents with MMR\n",
    "\n",
    "ğŸ”§ **Production Features**  \n",
    "- Retry policies & error handling\n",
    "- State checkpointing & persistence\n",
    "- LLM caching for performance\n",
    "- Flexible: Works with or without RAG\n",
    "- Gradio UI with RAG enabled\n",
    "- Streaming support\n",
    "- Thread-based conversation tracking\n",
    "\n",
    "\n",
    "### Benefits of RAG Integration\n",
    "\n",
    "âœ… **Reduced Hallucination**: Responses grounded in actual documents  \n",
    "âœ… **Domain-Specific Knowledge**: Access to specialized information beyond LLM training  \n",
    "âœ… **Up-to-Date Information**: Retrieves from current document base  \n",
    "âœ… **Transparent Sourcing**: Can trace responses back to source documents  \n",
    "âœ… **Improved Accuracy**: Combines LLM reasoning with factual retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```\n",
    "User Query â†’ Analyzer â†’ Planner â†’ Executor (with RAG) â†’ Evaluator\n",
    "                                          â†“\n",
    "                                    Retriever.invoke()\n",
    "                                          â†“\n",
    "                                  Vector Store (Chroma)\n",
    "                                          â†“\n",
    "                            Retrieved Documents (Top-3 MMR)\n",
    "                                          â†“\n",
    "                            Synthesize with Plan + Query\n",
    "                                          â†“\n",
    "                                    LLM Response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Quick Start Guide\n",
    "\n",
    "### Usage\n",
    "```python\n",
    "# Initialize retriever from your vector store\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# Create workflow with RAG enabled\n",
    "workflow_rag = AgenticWorkflow(\n",
    "    max_iterations=3,\n",
    "    retriever=retriever  # â† RAG enabled\n",
    ")\n",
    "\n",
    "# Responses will be grounded in your knowledge base\n",
    "response = workflow_rag.get_response(\"Question about your documents\")\n",
    "```\n",
    "\n",
    "### Available Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `model_id` | str | `meta-llama/Llama-3.1-8B-Instruct` | HuggingFace model to use |\n",
    "| `max_iterations` | int | `5` | Max refinement cycles (1-10 recommended) |\n",
    "| `temperature` | float | `0.5` | Creativity (0.0=deterministic, 1.0=creative) |\n",
    "| `max_new_tokens` | int | `512` | Max response length (256-2048) |\n",
    "| `enable_checkpointing` | bool | `True` | Enable state persistence |\n",
    "| `retry_on_error` | bool | `True` | Auto-retry failed nodes |\n",
    "| `retriever` | Optional[Any] | `None` | **LangChain retriever for RAG** |\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "1. **Update file paths**: Set your PDF file path in `pdf_path` and Chroma database directory in `persist_directory`\n",
    "2. **Run the test cells** to validate RAG integration\n",
    "3. **Adjust retriever parameters** (`k`, `fetch_k`, `lambda_mult`) based on results\n",
    "4. **Experiment with search types**: Try `\"similarity\"` or `\"similarity_score_threshold\"`\n",
    "5. **Monitor performance**: Track retrieval latency and response quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Dependencies\n",
    "\n",
    "Loading environment variables and configuring LangChain/LangGraph infrastructure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement operator (from versions: none)\n",
      "ERROR: No matching distribution found for operator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "âœ… Environment and caching configured\n",
      "âœ… Huggingface Inference client configured\n",
      "âœ… Environment and caching configured\n",
      "âœ… Huggingface Inference client configured\n"
     ]
    }
   ],
   "source": [
    "# Using InferenceClient with chat.completions API (compatible with more providers)\n",
    "%pip install -q -U huggingface-hub langchain-huggingface python-dotenv gradio langgraph langchain-core langchain-community operator typing-extensions functools langchain-chroma sentence-transformers\n",
    "\n",
    "# Install the langchain and pypdf packages\n",
    "%pip -q install -U pypdf langchain-chroma langchain-community langchain-text-splitters\n",
    "# Import required modules\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "#from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import operator\n",
    "from typing import TypedDict, Annotated, List, Optional, Dict, Any\n",
    "from functools import lru_cache\n",
    "load_dotenv()\n",
    "sys.path.append('..')\n",
    "from huggingface_hub import InferenceClient\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.types import RetryPolicy\n",
    "from langchain_community.cache import InMemoryCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import gradio as gr\n",
    "print('âœ… Environment and caching configured')\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "client = InferenceClient(token=hf_token)\n",
    "llama_model = \"meta-llama/Llama-3.1-8B-Instruct\"  \n",
    "print('âœ… Huggingface Inference client configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Vector Store Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector store retriever initialized\n",
      "   â””â”€ Search type: MMR\n",
      "   â””â”€ Top-k results: 3\n",
      "   â””â”€ Embedding model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "pdf_path = r\"Your Path\"\n",
    "\n",
    "# Load the PDF document\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)  # Adjust overlap as needed\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize the all-MiniLM-L6-v2 embedding model\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create a Chroma vector store and embed the chunks\n",
    "vector_store = Chroma.from_documents(\n",
    "    split_documents, \n",
    "    embedding=embedding_function,\n",
    "    persist_directory=\"Your Directory\"\n",
    ")\n",
    "\n",
    "# Create a retriever with optimized search parameters\n",
    "# Using MMR (Maximum Marginal Relevance) for diverse results\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",  # Maximum Marginal Relevance for diverse results\n",
    "    search_kwargs={\n",
    "        \"k\": 3,  # Retrieve top 3 most relevant documents\n",
    "        \"fetch_k\": 10,  # Fetch 10 candidates before MMR filtering\n",
    "        \"lambda_mult\": 0.7  # Balance between relevance (1.0) and diversity (0.0)\n",
    "    }\n",
    ")\n",
    "\n",
    "print('âœ… Vector store retriever initialized')\n",
    "print(f'   â””â”€ Search type: MMR')\n",
    "print(f'   â””â”€ Top-k results: 3')\n",
    "print(f'   â””â”€ Embedding model: all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent State with Reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enhanced state defined\n"
     ]
    }
   ],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    state of the agent workflow.\n",
    "\n",
    "    Uses annotated types with reducer functions for proper state management.\n",
    "    \"\"\"\n",
    "    # Use add_messages reducer for proper message handling\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    next_action: Annotated[str, operator.add]\n",
    "    iterations: Annotated[int, operator.add]\n",
    "    context: Annotated[Dict[str, Any], lambda x,y: {**x, **y}]\n",
    "    error_count: Annotated[int, operator.add]\n",
    "\n",
    "print('âœ… Enhanced state defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticWorkflow:\n",
    "    \"\"\"\n",
    "    Agentic workflow with best practices from LangGraph and LangChain.\n",
    "    Components:\n",
    "    - State management with proper reducer functions\n",
    "    - Error handling with retry policies\n",
    "    - Performance optimization through caching\n",
    "    - Checkpointing for fault tolerance\n",
    "    - Efficient prompt engineering\n",
    "    - Parallel execution support\n",
    "    - HuggingFace InferenceClient integration (OpenAI-compatible API)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str = llama_model,\n",
    "        max_iterations: int = 5,\n",
    "        temperature: float = 0.5,\n",
    "        max_new_tokens: int = 1024,\n",
    "        enable_checkpointing: bool = True,\n",
    "        retry_on_error: bool = True,\n",
    "        retriever: Optional[Any] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the enhanced agentic workflow.\n",
    "        \n",
    "        Args:\n",
    "            model_id: HuggingFace model identifier (e.g., \"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "            max_iterations: Maximum number of workflow iterations\n",
    "            temperature: LLM temperature for response variability\n",
    "            max_new_tokens: Maximum tokens to generate\n",
    "            enable_checkpointing: Enable state persistence\n",
    "            retry_on_error: Enable automatic retries on failures\n",
    "            retriever: LangChain retriever for RAG (Retrieval-Augmented Generation)\n",
    "        \"\"\"\n",
    "        self.max_iterations = max_iterations\n",
    "        self.retry_on_error = retry_on_error\n",
    "        self.model_id = model_id\n",
    "        self.temperature = temperature\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.retriever = retriever\n",
    "        \n",
    "        # Initialize HuggingFace InferenceClient with optimized settings\n",
    "        try:\n",
    "            self.llm = client\n",
    "            print(f\"âœ… Initialized HuggingFace InferenceClient with model: {model_id}\")\n",
    "            if self.retriever:\n",
    "                print(f\"âœ… RAG enabled with retriever integration\")\n",
    "        except Exception as e:\n",
    "            raise\n",
    "        \n",
    "        # Initialize checkpointer for state persistence\n",
    "        self.checkpointer = InMemorySaver() if enable_checkpointing else None\n",
    "        \n",
    "        # Build the workflow graph\n",
    "        self.graph = self._build_graph()\n",
    "        print(\"âœ… Workflow graph compiled successfully\")\n",
    "    \n",
    "    def _build_graph(self):\n",
    "        \"\"\"\n",
    "        Build the enhanced LangGraph workflow with retry policies.\n",
    "        \n",
    "        Returns:\n",
    "            Compiled StateGraph instance with checkpointing\n",
    "        \"\"\"\n",
    "        # Create workflow graph with enhanced state schema\n",
    "        workflow = StateGraph(state_schema=AgentState)\n",
    "        \n",
    "        # Define retry policy for nodes\n",
    "        retry_policy = RetryPolicy(\n",
    "            max_attempts=3,\n",
    "            retry_on=Exception  # Retry on any exception\n",
    "        ) if self.retry_on_error else None\n",
    "        \n",
    "        # Add nodes with retry policies\n",
    "        workflow.add_node(\n",
    "            \"analyzer\", \n",
    "            self._analyze_query,\n",
    "            retry_policy=retry_policy\n",
    "        )\n",
    "        workflow.add_node(\n",
    "            \"planner\", \n",
    "            self._plan_response,\n",
    "            retry_policy=retry_policy\n",
    "        )\n",
    "        workflow.add_node(\n",
    "            \"executor\", \n",
    "            self._execute_plan,\n",
    "            retry_policy=retry_policy\n",
    "        )\n",
    "        workflow.add_node(\n",
    "            \"evaluator\", \n",
    "            self._evaluate_result,\n",
    "            retry_policy=retry_policy\n",
    "        )\n",
    "        \n",
    "        # Define entry point\n",
    "        workflow.set_entry_point(\"analyzer\")\n",
    "        \n",
    "        # Add edges\n",
    "        workflow.add_edge(\"analyzer\", \"planner\")\n",
    "        workflow.add_edge(\"planner\", \"executor\")\n",
    "        workflow.add_edge(\"executor\", \"evaluator\")\n",
    "        \n",
    "        # Add conditional routing from evaluator\n",
    "        workflow.add_conditional_edges(\n",
    "            \"evaluator\",\n",
    "            self._should_continue,\n",
    "            {\n",
    "                \"continue\": \"analyzer\",\n",
    "                \"end\": END\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Compile with checkpointing\n",
    "        return workflow.compile(checkpointer=self.checkpointer)\n",
    "    \n",
    "    @lru_cache(maxsize=128)\n",
    "    def _get_system_prompt(self, task: str) -> str:\n",
    "        \"\"\"\n",
    "        Get cached system prompts for different tasks.\n",
    "        \n",
    "        Args:\n",
    "            task: The task type (analyze, plan, execute, evaluate)\n",
    "            \n",
    "        Returns:\n",
    "            System prompt for the task\n",
    "        \"\"\"\n",
    "        prompts = {\n",
    "            \"analyze\": \"You are an expert analyst. Carefully examine queries and identify core intents concisely.\",\n",
    "            \"plan\": \"You are a strategic planner. Create clear, actionable step-by-step plans.\",\n",
    "            \"execute\": \"You are a knowledgeable assistant. Provide comprehensive, accurate answers.\",\n",
    "            \"evaluate\": \"You are a quality assessor. Evaluate responses objectively and concisely.\"\n",
    "        }\n",
    "        return prompts.get(task, \"You are a helpful AI assistant.\")\n",
    "    \n",
    "    def _handle_llm_call(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        system_prompt: Optional[str] = None,\n",
    "        max_retries: int = 3\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Robust LLM call with error handling and retries.\n",
    "        Args:\n",
    "            prompt: The user prompt\n",
    "            system_prompt: Optional system prompt\n",
    "            max_retries: Maximum number of retry attempts\n",
    "            \n",
    "        Returns:\n",
    "            LLM response text\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If all retries fail\n",
    "        \"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Construct messages for HuggingFace InferenceClient (OpenAI-compatible format)\n",
    "                messages = []\n",
    "                \n",
    "                # Add system message if provided\n",
    "                if system_prompt:\n",
    "                    messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "                \n",
    "                # Add user message\n",
    "                messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "                \n",
    "                # Call HuggingFace InferenceClient's chat.completions.create() method\n",
    "                response = self.llm.chat.completions.create(\n",
    "                    model=self.model_id,\n",
    "                    messages=messages,\n",
    "                    temperature=self.temperature,\n",
    "                    max_tokens=self.max_new_tokens\n",
    "                )\n",
    "                \n",
    "                # Extract content from response\n",
    "                return response.choices[0].message.content\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  LLM call attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"âŒ All {max_retries} LLM call attempts failed\")\n",
    "                    raise\n",
    "        \n",
    "        return \"Error: Failed to get LLM response\"\n",
    "    \n",
    "    def _analyze_query(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze the user query to understand intent with improved prompting.\n",
    "        \n",
    "        Args:\n",
    "            state: Current agent state\n",
    "            \n",
    "        Returns:\n",
    "            State update dict\n",
    "        \"\"\"\n",
    "        try:\n",
    "            messages = state[\"messages\"]\n",
    "            last_message = messages[-1].content if messages else \"\"\n",
    "            \n",
    "            # Optimized prompt engineering\n",
    "            system_prompt = self._get_system_prompt(\"analyze\")\n",
    "            prompt = f\"\"\"Query: {last_message}\n",
    "\n",
    "Task: Identify the main intent in 1-2 sentences. Focus on what the user wants to accomplish.\n",
    "\n",
    "Analysis:\"\"\"\n",
    "            \n",
    "            response = self._handle_llm_call(prompt, system_prompt)\n",
    "            \n",
    "            # Return state update\n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Analysis: {response}\")],\n",
    "                \"next_action\": \"plan\",\n",
    "                \"context\": {\"last_analysis\": response}\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Analysis failed: {e}\")\n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Analysis Error: {str(e)}\")],\n",
    "                \"next_action\": \"end\",\n",
    "                \"error_count\": 1\n",
    "            }\n",
    "    \n",
    "    def _plan_response(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Plan the response based on the analysis with concise prompting.\n",
    "        \n",
    "        Args:\n",
    "            state: Current agent state\n",
    "            \n",
    "        Returns:\n",
    "            State update dict\n",
    "        \"\"\"\n",
    "        try:\n",
    "            messages = state[\"messages\"]\n",
    "            \n",
    "            # Get only relevant context (last 3 messages)\n",
    "            context = \"\\n\".join([str(m.content) for m in messages[-3:]])\n",
    "            \n",
    "            system_prompt = self._get_system_prompt(\"plan\")\n",
    "            prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Task: Create a brief 2-3 step plan to address the user's request.\n",
    "\n",
    "Plan:\"\"\"\n",
    "            \n",
    "            response = self._handle_llm_call(prompt, system_prompt)\n",
    "            \n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Plan: {response}\")],\n",
    "                \"next_action\": \"execute\",\n",
    "                \"context\": {\"last_plan\": response}\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Planning failed: {e}\")\n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Planning Error: {str(e)}\")],\n",
    "                \"next_action\": \"end\",\n",
    "                \"error_count\": 1\n",
    "            }\n",
    "    \n",
    "    def _execute_plan(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the planned response with RAG (Retrieval-Augmented Generation).\n",
    "        \n",
    "        Args:\n",
    "            state: Current agent state\n",
    "            \n",
    "        Returns:\n",
    "            State update dict\n",
    "        \"\"\"\n",
    "        try:\n",
    "            messages = state[\"messages\"]\n",
    "            original_query = messages[0].content\n",
    "            \n",
    "            # Use context from state if available\n",
    "            plan = state.get(\"context\", {}).get(\"last_plan\", \"\")\n",
    "            if not plan:\n",
    "                plan = messages[-1].content if messages else \"\"\n",
    "            \n",
    "            # RAG Integration: Retrieve relevant documents if retriever is available\n",
    "            context_text = \"\"\n",
    "            if self.retriever:\n",
    "                try:\n",
    "                    # Retrieve relevant documents from vector store\n",
    "                    retrieved_docs = self.retriever.invoke(original_query)\n",
    "                    \n",
    "                    # Format retrieved documents as context\n",
    "                    if retrieved_docs:\n",
    "                        context_text = \"\\n\\n\".join([\n",
    "                            f\"Document {i+1}:\\n{doc.page_content}\" \n",
    "                            for i, doc in enumerate(retrieved_docs)\n",
    "                        ])\n",
    "                        print(f\"ğŸ“š Retrieved {len(retrieved_docs)} relevant documents from vector store\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Retrieval failed: {e}. Proceeding without RAG.\")\n",
    "                    context_text = \"\"\n",
    "            \n",
    "            system_prompt = self._get_system_prompt(\"execute\")\n",
    "            \n",
    "            # Build prompt with or without retrieved context\n",
    "            if context_text:\n",
    "                prompt = f\"\"\"Original Query: {original_query}\n",
    "\n",
    "Plan: {plan}\n",
    "\n",
    "Retrieved Context from Knowledge Base:\n",
    "{context_text}\n",
    "\n",
    "Task: Synthesize the plan with the retrieved context to provide a comprehensive, accurate answer to the original query. Use the retrieved documents to support your response with specific information.\n",
    "\n",
    "Answer:\"\"\"\n",
    "            else:\n",
    "                prompt = f\"\"\"Original Query: {original_query}\n",
    "\n",
    "Plan: {plan}\n",
    "\n",
    "Task: Provide a comprehensive, accurate answer to the original query.\n",
    "\n",
    "Answer:\"\"\"\n",
    "            \n",
    "            response = self._handle_llm_call(prompt, system_prompt)\n",
    "            \n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Result: {response}\")],\n",
    "                \"next_action\": \"evaluate\",\n",
    "                \"context\": {\"last_result\": response}\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Execution failed: {e}\")\n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Execution Error: {str(e)}\")],\n",
    "                \"next_action\": \"end\",\n",
    "                \"error_count\": 1\n",
    "            }\n",
    "    \n",
    "    def _evaluate_result(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate if the result is satisfactory with improved criteria.\n",
    "        \n",
    "        Args:\n",
    "            state: Current agent state\n",
    "            \n",
    "        Returns:\n",
    "            State update dict\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Increment iterations\n",
    "            current_iterations = state.get(\"iterations\", 0) + 1\n",
    "            \n",
    "            messages = state[\"messages\"]\n",
    "            result = state.get(\"context\", {}).get(\"last_result\", \"\")\n",
    "            if not result:\n",
    "                result = messages[-1].content if messages else \"\"\n",
    "            \n",
    "            system_prompt = self._get_system_prompt(\"evaluate\")\n",
    "            prompt = f\"\"\"Result: {result}\n",
    "\n",
    "Task: Evaluate if this result is satisfactory. Respond ONLY with:\n",
    "- 'SATISFACTORY' if the answer is complete and accurate\n",
    "- 'NEEDS_IMPROVEMENT' if it requires refinement\n",
    "\n",
    "Evaluation:\"\"\"\n",
    "            \n",
    "            response = self._handle_llm_call(prompt, system_prompt)\n",
    "            \n",
    "            # Determine next action\n",
    "            if \"SATISFACTORY\" in response.upper() or current_iterations >= self.max_iterations:\n",
    "                next_action = \"end\"\n",
    "            else:\n",
    "                next_action = \"continue\"\n",
    "            \n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Evaluation: {response}\")],\n",
    "                \"next_action\": next_action,\n",
    "                \"iterations\": 1,  # Increment by 1 (operator.add)\n",
    "                \"context\": {\"last_evaluation\": response}\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Evaluation failed: {e}\")\n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Evaluation Error: {str(e)}\")],\n",
    "                \"next_action\": \"end\",\n",
    "                \"iterations\": 1,\n",
    "                \"error_count\": 1\n",
    "            }\n",
    "    from typing import Literal\n",
    "    def _should_continue(self, state: AgentState) -> Literal[\"continue\", \"end\"]:\n",
    "        \"\"\"\n",
    "        Decide whether to continue or end the workflow with enhanced logic.\n",
    "        \n",
    "        Args:\n",
    "            state: Current agent state\n",
    "            \n",
    "        Returns:\n",
    "            'continue' or 'end'\n",
    "        \"\"\"\n",
    "        # Check error threshold\n",
    "        if state.get(\"error_count\", 0) >= 2:\n",
    "            print(\"âš ï¸  Too many errors, ending workflow\")\n",
    "            return \"end\"\n",
    "        \n",
    "        # Check iteration limit\n",
    "        if state.get(\"iterations\", 0) >= self.max_iterations:\n",
    "            print(f\"â„¹ï¸  Reached max iterations ({self.max_iterations})\")\n",
    "            return \"end\"\n",
    "        \n",
    "        # Check next_action\n",
    "        next_action = state.get(\"next_action\", \"end\")\n",
    "        if next_action == \"continue\":\n",
    "            print(\"ğŸ”„ Continuing workflow for improvement\")\n",
    "            return \"continue\"\n",
    "        \n",
    "        print(\"âœ… Workflow complete\")\n",
    "        return \"end\"\n",
    "    \n",
    "    def run(\n",
    "        self, \n",
    "        query: str, \n",
    "        thread_id: Optional[str] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run the agentic workflow with a user query.\n",
    "        \n",
    "        Args:\n",
    "            query: User input query\n",
    "            thread_id: Optional thread ID for checkpointing\n",
    "            \n",
    "        Returns:\n",
    "            Final state of the workflow\n",
    "        \"\"\"\n",
    "        # Initial state\n",
    "        initial_state: AgentState = {\n",
    "            \"messages\": [HumanMessage(content=query)],\n",
    "            \"next_action\": \"analyze\",\n",
    "            \"iterations\": 0,\n",
    "            \"context\": {},\n",
    "            \"error_count\": 0\n",
    "        }\n",
    "        \n",
    "        # Configuration for checkpointing\n",
    "        config = {\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": thread_id or \"default\"\n",
    "            }\n",
    "        } if self.checkpointer else {}\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸš€ Starting workflow for query: {query[:50]}...\")\n",
    "            final_state = self.graph.invoke(initial_state, config=config)\n",
    "            print(\"âœ… Workflow completed successfully\")\n",
    "            return final_state\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Workflow failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def stream(\n",
    "        self, \n",
    "        query: str, \n",
    "        thread_id: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Stream the workflow execution for real-time updates.\n",
    "        \n",
    "        Args:\n",
    "            query: User input query\n",
    "            thread_id: Optional thread ID for checkpointing\n",
    "            \n",
    "        Yields:\n",
    "            State updates as they occur\n",
    "        \"\"\"\n",
    "        initial_state: AgentState = {\n",
    "            \"messages\": [HumanMessage(content=query)],\n",
    "            \"next_action\": \"analyze\",\n",
    "            \"iterations\": 0,\n",
    "            \"context\": {},\n",
    "            \"error_count\": 0\n",
    "        }\n",
    "        \n",
    "        config = {\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": thread_id or \"default\"\n",
    "            }\n",
    "        } if self.checkpointer else {}\n",
    "        \n",
    "        try:\n",
    "            for chunk in self.graph.stream(initial_state, config=config, stream_mode=\"updates\"):\n",
    "                yield chunk\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Streaming failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_response(self, query: str, thread_id: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Get a simple text response from the workflow.\n",
    "        \n",
    "        Args:\n",
    "            query: User input query\n",
    "            thread_id: Optional thread ID for checkpointing\n",
    "            \n",
    "        Returns:\n",
    "            Final response text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self.run(query, thread_id)\n",
    "            \n",
    "            # Extract the final meaningful response\n",
    "            messages = result.get(\"messages\", [])\n",
    "            for msg in reversed(messages):\n",
    "                if isinstance(msg, AIMessage) and msg.content.startswith(\"Result:\"):\n",
    "                    return msg.content.replace(\"Result:\", \"\").strip()\n",
    "            \n",
    "            return \"No response generated.\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to get response: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RAG-Enabled Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AgenticWorkflow with RAG integration\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ§ª Testing RAG-Enabled Agentic Workflow\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize workflow with retriever\n",
    "workflow_rag = AgenticWorkflow(\n",
    "    max_iterations=3,\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1024,\n",
    "    enable_checkpointing=True,\n",
    "    retry_on_error=True,\n",
    "    retriever=retriever  # â† RAG enabled with retriever\n",
    ")\n",
    "\n",
    "# Test query that should benefit from RAG\n",
    "test_query = \"What is the attention mechanism in transformers?\"\n",
    "\n",
    "print(f\"\\nğŸ“ Query: {test_query}\\n\")\n",
    "print(\"ğŸ”„ Running workflow with RAG enabled...\\n\")\n",
    "\n",
    "response = workflow_rag.get_response(\n",
    "    test_query,\n",
    "    thread_id=\"rag_test_1\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… FINAL RESPONSE WITH RAG:\")\n",
    "print(\"=\" * 80)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Workflow with custom queries using Gradio user interface (RAG-Enabled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_workflow(message: str, history: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Chat function that integrates AgenticWorkflow with Gradio.\n",
    "    RAG-enabled by default to provide grounded responses.\n",
    "    \n",
    "    Args:\n",
    "        message: User's input message\n",
    "        history: Conversation history (not used in current implementation)\n",
    "        \n",
    "    Returns:\n",
    "        Final response from the workflow\n",
    "    \"\"\"\n",
    "    # Create workflow instance with RAG enabled\n",
    "    workflow = AgenticWorkflow(\n",
    "        max_iterations=3,\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=1024,\n",
    "        enable_checkpointing=True,\n",
    "        retry_on_error=True,\n",
    "        retriever=retriever  # â† RAG enabled with retriever\n",
    "    )\n",
    "    \n",
    "    # Get response using the workflow\n",
    "    try:\n",
    "        response = workflow.get_response(message, thread_id=\"gradio_session\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error processing request: {str(e)}\"\n",
    "\n",
    "# Create Gradio chat interface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_with_workflow,\n",
    "    type=\"messages\",\n",
    "    title=\"Agentic Workflow Chat (RAG-Enabled)\",\n",
    "    description=\"Chat with an AI agent that analyzes, plans, executes with RAG, and evaluates responses. Responses are grounded in the knowledge base (transformer paper).\",\n",
    "    examples=[\n",
    "        \"Explain how Transformers differs from previous architectures?\",\n",
    "        \"What is the attention mechanism in transformers?\",\n",
    "        \"Explain the key innovations in the attention is all you need paper\",\n",
    "        \"What are the key components of a Transformer model?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Verify Inter-Node Communication\n",
    "\n",
    "**What This Test Does:**\n",
    "\n",
    "This test explicitly shows the **actual data flow** in a clean, single-pass format:\n",
    "how nodes pass information to each other through the state.\n",
    "\n",
    "âœ… **INPUT**: What specific data each node reads  \n",
    "âœ… **OUTPUT**: What each node produces  \n",
    "âœ… **STORES**: What gets saved in `context` for the next node  \n",
    "âœ… **ROUTES**: Where the workflow goes next\n",
    "\n",
    "**Key Points:**\n",
    "- Shows the **exact content** being passed between nodes\n",
    "- Tracks how `context` dict grows as state flows through the workflow\n",
    "- Makes it crystal clear that nodes read **specific parts** of the state\n",
    "\n",
    "**Run the test below to see the clean, simplified data flow!** ğŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ” INTER-NODE COMMUNICATION TEST\n",
      "================================================================================\n",
      "\n",
      "This test shows EXACTLY what each node receives and produces.\n",
      "\n",
      "âœ… Initialized HuggingFace InferenceClient with model: meta-llama/Llama-3.1-8B-Instruct\n",
      "âœ… Workflow graph compiled successfully\n",
      "ğŸ“ Original Query: 'What is machine learning?'\n",
      "\n",
      "================================================================================\n",
      "STEP 1: ANALYZER\n",
      "================================================================================\n",
      "1.ANALYZER reads user query â†’ produces analysis â†’ stores in context\n",
      "ğŸ“¥ INPUT:  'What is machine learning?'\n",
      "ğŸ“¤ OUTPUT: 'Analysis: The main intent of this query is to gain a general understan...'\n",
      "ğŸ’¾ STORES: context['last_analysis'] = 'The main intent of this query is to gain a general understanding or de...'\n",
      "ğŸ”€ ROUTES: â†’ plan\n",
      "\n",
      "================================================================================\n",
      "STEP 1: ANALYZER\n",
      "================================================================================\n",
      "1.ANALYZER reads user query â†’ produces analysis â†’ stores in context\n",
      "ğŸ“¥ INPUT:  'What is machine learning?'\n",
      "ğŸ“¤ OUTPUT: 'Analysis: The main intent of this query is to gain a general understan...'\n",
      "ğŸ’¾ STORES: context['last_analysis'] = 'The main intent of this query is to gain a general understanding or de...'\n",
      "ğŸ”€ ROUTES: â†’ plan\n",
      "\n",
      "================================================================================\n",
      "STEP 2: PLANNER\n",
      "================================================================================\n",
      "2. PLANNER reads analysis from messages[-3:] â†’ produces plan â†’ stores in context\n",
      "ğŸ“¥ INPUT:  messages[-3:] contains Analyzer's: 'Analysis: The main intent of this query is to gain a general understan...'\n",
      "ğŸ“¤ OUTPUT: 'Plan: **Plan: Explaining Machine Learning**\n",
      "\n",
      "**Step 1: Define Machine ...'\n",
      "ğŸ’¾ STORES: context['last_plan'] = '**Plan: Explaining Machine Learning**\n",
      "\n",
      "**Step 1: Define Machine Learni...'\n",
      "ğŸ”€ ROUTES: â†’ execute\n",
      "\n",
      "================================================================================\n",
      "STEP 2: PLANNER\n",
      "================================================================================\n",
      "2. PLANNER reads analysis from messages[-3:] â†’ produces plan â†’ stores in context\n",
      "ğŸ“¥ INPUT:  messages[-3:] contains Analyzer's: 'Analysis: The main intent of this query is to gain a general understan...'\n",
      "ğŸ“¤ OUTPUT: 'Plan: **Plan: Explaining Machine Learning**\n",
      "\n",
      "**Step 1: Define Machine ...'\n",
      "ğŸ’¾ STORES: context['last_plan'] = '**Plan: Explaining Machine Learning**\n",
      "\n",
      "**Step 1: Define Machine Learni...'\n",
      "ğŸ”€ ROUTES: â†’ execute\n",
      "\n",
      "================================================================================\n",
      "STEP 3: EXECUTOR\n",
      "================================================================================\n",
      "3. EXECUTOR reads plan from context + original query â†’ produces result â†’ stores in context\n",
      "ğŸ“¥ INPUT:  context['last_plan'] = '**Plan: Explaining Machine Learning**\n",
      "\n",
      "**Step 1: Define Machine Learni...'\n",
      "           messages[0] = 'What is machine learning?'\n",
      "ğŸ“¤ OUTPUT: 'Result: **What is Machine Learning?**\n",
      "\n",
      "Machine learning is a subset of...'\n",
      "ğŸ’¾ STORES: context['last_result'] = '**What is Machine Learning?**\n",
      "\n",
      "Machine learning is a subset of artific...'\n",
      "ğŸ”€ ROUTES: â†’ evaluate\n",
      "\n",
      "================================================================================\n",
      "STEP 3: EXECUTOR\n",
      "================================================================================\n",
      "3. EXECUTOR reads plan from context + original query â†’ produces result â†’ stores in context\n",
      "ğŸ“¥ INPUT:  context['last_plan'] = '**Plan: Explaining Machine Learning**\n",
      "\n",
      "**Step 1: Define Machine Learni...'\n",
      "           messages[0] = 'What is machine learning?'\n",
      "ğŸ“¤ OUTPUT: 'Result: **What is Machine Learning?**\n",
      "\n",
      "Machine learning is a subset of...'\n",
      "ğŸ’¾ STORES: context['last_result'] = '**What is Machine Learning?**\n",
      "\n",
      "Machine learning is a subset of artific...'\n",
      "ğŸ”€ ROUTES: â†’ evaluate\n",
      "\n",
      "â„¹ï¸  Reached max iterations (1)\n",
      "================================================================================\n",
      "STEP 4: EVALUATOR\n",
      "================================================================================\n",
      "   4. EVALUATOR reads result from context â†’ produces evaluation â†’ routes to end\n",
      "ğŸ“¥ INPUT:  context['last_result'] = '**What is Machine Learning?**\n",
      "\n",
      "Machine learning is a subset of artific...'\n",
      "ğŸ“¤ OUTPUT: 'Evaluation: SATISFACTORY...'\n",
      "ğŸ’¾ STORES: context['last_evaluation'] = 'SATISFACTORY...'\n",
      "ğŸ”€ ROUTES: â†’ end\n",
      "\n",
      "================================================================================\n",
      "âœ… COMMUNICATION FLOW VERIFIED\n",
      "================================================================================\n",
      "\n",
      "ğŸ”— Summary:\n",
      "   1. ANALYZER reads user query â†’ produces analysis â†’ stores in context\n",
      "   2. PLANNER reads analysis from messages[-3:] â†’ produces plan â†’ stores in context\n",
      "   3. EXECUTOR reads plan from context + original query â†’ produces result â†’ stores in context\n",
      "   4. EVALUATOR reads result from context â†’ produces evaluation â†’ routes to end\n",
      "\n",
      "âœ¨ Each node explicitly uses the previous node's output!\n",
      "â„¹ï¸  Reached max iterations (1)\n",
      "================================================================================\n",
      "STEP 4: EVALUATOR\n",
      "================================================================================\n",
      "   4. EVALUATOR reads result from context â†’ produces evaluation â†’ routes to end\n",
      "ğŸ“¥ INPUT:  context['last_result'] = '**What is Machine Learning?**\n",
      "\n",
      "Machine learning is a subset of artific...'\n",
      "ğŸ“¤ OUTPUT: 'Evaluation: SATISFACTORY...'\n",
      "ğŸ’¾ STORES: context['last_evaluation'] = 'SATISFACTORY...'\n",
      "ğŸ”€ ROUTES: â†’ end\n",
      "\n",
      "================================================================================\n",
      "âœ… COMMUNICATION FLOW VERIFIED\n",
      "================================================================================\n",
      "\n",
      "ğŸ”— Summary:\n",
      "   1. ANALYZER reads user query â†’ produces analysis â†’ stores in context\n",
      "   2. PLANNER reads analysis from messages[-3:] â†’ produces plan â†’ stores in context\n",
      "   3. EXECUTOR reads plan from context + original query â†’ produces result â†’ stores in context\n",
      "   4. EVALUATOR reads result from context â†’ produces evaluation â†’ routes to end\n",
      "\n",
      "âœ¨ Each node explicitly uses the previous node's output!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¬ Deep Inspection: Track How Nodes Communicate\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ” INTER-NODE COMMUNICATION TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nThis test shows EXACTLY what each node receives and produces.\\n\")\n",
    "\n",
    "# Create workflow with verbose tracking\n",
    "workflow_debug = AgenticWorkflow(\n",
    "    max_iterations=1,  # Single pass to see clear communication\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=256\n",
    ")\n",
    "\n",
    "test_query = \"What is machine learning?\"\n",
    "print(f\"ğŸ“ Original Query: '{test_query}'\\n\")\n",
    "\n",
    "# Track cumulative state to show what gets passed between nodes\n",
    "cumulative_state = {\"messages\": [], \"context\": {}}\n",
    "step_num = 0\n",
    "\n",
    "for update in workflow_debug.stream(test_query, thread_id=\"comm_test\"):\n",
    "    step_num += 1\n",
    "    node_name = list(update.keys())[0]\n",
    "    node_output = update[node_name]\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"STEP {step_num}: {node_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Show what this node READS\n",
    "    if node_name == \"analyzer\":\n",
    "        print(\"1.ANALYZER reads user query â†’ produces analysis â†’ stores in context\")\n",
    "        print(f\"ğŸ“¥ INPUT:  '{test_query}'\")\n",
    "\n",
    "        \n",
    "    elif node_name == \"planner\":\n",
    "        # Planner reads last 3 messages\n",
    "        print(\"2. PLANNER reads analysis from messages[-3:] â†’ produces plan â†’ stores in context\")\n",
    "        analyzer_msg = cumulative_state[\"messages\"][-1].content if cumulative_state[\"messages\"] else \"\"\n",
    "        print(f\"ğŸ“¥ INPUT:  messages[-3:] contains Analyzer's: '{analyzer_msg[:70]}...'\")\n",
    "\n",
    "        \n",
    "    elif node_name == \"executor\":\n",
    "        print(\"3. EXECUTOR reads plan from context + original query â†’ produces result â†’ stores in context\")\n",
    "        # Executor reads plan from context\n",
    "        plan = cumulative_state[\"context\"].get(\"last_plan\", \"\")\n",
    "        print(f\"ğŸ“¥ INPUT:  context['last_plan'] = '{plan[:70]}...'\")\n",
    "        print(f\"           messages[0] = '{test_query}'\")\n",
    "\n",
    "        \n",
    "    elif node_name == \"evaluator\":\n",
    "        print(\"   4. EVALUATOR reads result from context â†’ produces evaluation â†’ routes to end\")\n",
    "        # Evaluator reads result from context\n",
    "        result = cumulative_state[\"context\"].get(\"last_result\", \"\")\n",
    "        print(f\"ğŸ“¥ INPUT:  context['last_result'] = '{result[:70]}...'\")\n",
    "\n",
    "    # Show what this node PRODUCES\n",
    "    if \"messages\" in node_output and node_output[\"messages\"]:\n",
    "        msg = node_output[\"messages\"][-1].content\n",
    "        print(f\"ğŸ“¤ OUTPUT: '{msg[:70]}...'\")\n",
    "    \n",
    "    # Show what this node STORES for next node\n",
    "    if \"context\" in node_output and node_output[\"context\"]:\n",
    "        for key, value in node_output[\"context\"].items():\n",
    "            print(f\"ğŸ’¾ STORES: context['{key}'] = '{str(value)[:70]}...'\")\n",
    "    \n",
    "    # Show routing\n",
    "    if \"next_action\" in node_output:\n",
    "        next_step = node_output[\"next_action\"]\n",
    "        print(f\"ğŸ”€ ROUTES: â†’ {next_step}\")\n",
    "    \n",
    "    print()  # Blank line for readability\n",
    "    \n",
    "    # Update cumulative state for next node\n",
    "    if \"messages\" in node_output:\n",
    "        cumulative_state[\"messages\"].extend(node_output[\"messages\"])\n",
    "    if \"context\" in node_output:\n",
    "        cumulative_state[\"context\"].update(node_output[\"context\"])\n",
    "    if \"next_action\" in node_output:\n",
    "        cumulative_state[\"next_action\"] = node_output[\"next_action\"]\n",
    "    if \"iterations\" in node_output:\n",
    "        cumulative_state[\"iterations\"] = cumulative_state.get(\"iterations\", 0) + node_output[\"iterations\"]\n",
    "    if \"error_count\" in node_output:\n",
    "        cumulative_state[\"error_count\"] = cumulative_state.get(\"error_count\", 0) + node_output[\"error_count\"]\n",
    "\n",
    "\n",
    "# Final summary\n",
    "print(f\"{'='*80}\")\n",
    "print(\"âœ… COMMUNICATION FLOW VERIFIED\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(\"ğŸ”— Summary:\")\n",
    "print(\"   1. ANALYZER reads user query â†’ produces analysis â†’ stores in context\")\n",
    "print(\"   2. PLANNER reads analysis from messages[-3:] â†’ produces plan â†’ stores in context\")\n",
    "print(\"   3. EXECUTOR reads plan from context + original query â†’ produces result â†’ stores in context\")\n",
    "print(\"   4. EVALUATOR reads result from context â†’ produces evaluation â†’ routes to end\")\n",
    "print(\"\\nâœ¨ Each node explicitly uses the previous node's output!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– How Nodes Communicate: Code Analysis (with RAG)\n",
    "\n",
    "### 1ï¸âƒ£ **Analyzer â†’ Planner** (via messages list)\n",
    "\n",
    "**Analyzer OUTPUT:**\n",
    "```python\n",
    "# In _analyze_query():\n",
    "return {\n",
    "    \"messages\": [AIMessage(content=f\"Analysis: {response}\")],\n",
    "    \"context\": {\"last_analysis\": response}  # â† Stored but not used by planner\n",
    "}\n",
    "```\n",
    "\n",
    "**Planner INPUT:**\n",
    "```python\n",
    "# In _plan_response():\n",
    "messages = state[\"messages\"]  # â† Gets ALL accumulated messages\n",
    "context = \"\\n\".join([str(m.content) for m in messages[-3:]])  # â† Reads last 3 (includes analyzer's output)\n",
    "```\n",
    "\n",
    "**What's passed**: Analyzer's analysis message  \n",
    "**How**: Via `messages` list (planner reads `messages[-3:]`)\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **Planner â†’ Executor** (via context dict)\n",
    "\n",
    "**Planner OUTPUT:**\n",
    "```python\n",
    "# In _plan_response():\n",
    "return {\n",
    "    \"messages\": [AIMessage(content=f\"Plan: {response}\")],\n",
    "    \"context\": {\"last_plan\": response}  # â† Stores plan for executor\n",
    "}\n",
    "```\n",
    "\n",
    "**Executor INPUT:**\n",
    "```python\n",
    "# In _execute_plan():\n",
    "plan = state.get(\"context\", {}).get(\"last_plan\", \"\")  # â† Reads planner's plan\n",
    "original_query = messages[0].content  # â† Also reads original query\n",
    "```\n",
    "\n",
    "**What's passed**: Planner's 2-3 step action plan + original query  \n",
    "**How**: Via `state[\"context\"][\"last_plan\"]` (structured storage)\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ **Executor â†’ Evaluator** (via context dict) **[RAG ENABLED]**\n",
    "\n",
    "**Executor OUTPUT (with RAG):**\n",
    "```python\n",
    "# In _execute_plan():\n",
    "# Step 1: Retrieve relevant documents\n",
    "if self.retriever:\n",
    "    retrieved_docs = self.retriever.invoke(original_query)  # â† RAG retrieval\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# Step 2: Synthesize with plan and retrieved context\n",
    "prompt = f\"\"\"Original Query: {original_query}\n",
    "Plan: {plan}\n",
    "Retrieved Context: {context_text}  # â† Grounded in knowledge base\n",
    "Task: Synthesize...\"\"\"\n",
    "\n",
    "response = self._handle_llm_call(prompt, system_prompt)\n",
    "\n",
    "return {\n",
    "    \"messages\": [AIMessage(content=f\"Result: {response}\")],\n",
    "    \"context\": {\"last_result\": response}  # â† Stores result for evaluator\n",
    "}\n",
    "```\n",
    "\n",
    "**Evaluator INPUT:**\n",
    "```python\n",
    "# In _evaluate_result():\n",
    "result = state.get(\"context\", {}).get(\"last_result\", \"\")  # â† Reads executor's result\n",
    "```\n",
    "\n",
    "**What's passed**: Executor's comprehensive answer (grounded in retrieved documents when RAG is enabled)  \n",
    "**How**: Via `state[\"context\"][\"last_result\"]` (structured storage)\n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ **Evaluator â†’ Analyzer** (loop back if needs improvement)\n",
    "\n",
    "**Evaluator OUTPUT:**\n",
    "```python\n",
    "# In _evaluate_result():\n",
    "return {\n",
    "    \"messages\": [AIMessage(content=f\"Evaluation: {response}\")],\n",
    "    \"next_action\": \"continue\" if needs_improvement else \"end\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Routing Decision:**\n",
    "```python\n",
    "# In _should_continue():\n",
    "if next_action == \"continue\":\n",
    "    return \"continue\"  # â† Routes back to analyzer\n",
    "```\n",
    "\n",
    "**What's passed**: Entire conversation history (all previous messages + context)  \n",
    "**How**: Full state preserved via LangGraph's state management\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”‘ Three Communication Patterns:\n",
    "\n",
    "1. **Messages List** (`messages[-3:]`): Used when node needs conversation context\n",
    "   - Planner uses this to read Analyzer's output\n",
    "   \n",
    "2. **Context Dict** (`context['key']`): Used for specific structured data\n",
    "   - Executor reads `last_plan` from Planner\n",
    "   - Evaluator reads `last_result` from Executor\n",
    "\n",
    "3. **Original Query** (`messages[0]`): Always accessible to all nodes\n",
    "   - **Executor uses it for RAG retrieval** via `retriever.invoke(original_query)`\n",
    "\n",
    "4. **RAG Retrieval** (when retriever is provided):\n",
    "   - Executor retrieves documents from vector store\n",
    "   - Synthesizes retrieved context with plan\n",
    "   - Produces grounded responses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Visual Data Flow Diagram\n",
    "\n",
    "```\n",
    "USER QUERY: \"What is the attention mechanism in transformers?\"\n",
    "      |\n",
    "      v\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ANALYZER NODE                                   â”‚\n",
    "â”‚  INPUT:  Original query                         â”‚\n",
    "â”‚  OUTPUT: \"Analysis: User wants to know about    â”‚\n",
    "â”‚          attention mechanism...\"                 â”‚\n",
    "â”‚  STORES: context['last_analysis'] = \"...\"       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â”‚\n",
    "                      v\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚   STATE PASSED:     â”‚\n",
    "            â”‚ - messages: [       â”‚\n",
    "            â”‚     HumanMessage,   â”‚\n",
    "            â”‚     AIMessage]      â”‚\n",
    "            â”‚ - context: {        â”‚\n",
    "            â”‚     last_analysis   â”‚\n",
    "            â”‚   }                 â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â”‚\n",
    "                      v\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  PLANNER NODE                                    â”‚\n",
    "â”‚  INPUT:  messages[-3:] â† Reads analyzer output  â”‚\n",
    "â”‚  OUTPUT: \"Plan: 1. Define attention 2. Explain  â”‚\n",
    "â”‚          mechanism 3. Provide examples...\"       â”‚\n",
    "â”‚  STORES: context['last_plan'] = \"...\"           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â”‚\n",
    "                      v\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚   STATE PASSED:     â”‚\n",
    "            â”‚ - messages: [       â”‚\n",
    "            â”‚     HumanMessage,   â”‚\n",
    "            â”‚     AIMessage,      â”‚\n",
    "            â”‚     AIMessage]      â”‚\n",
    "            â”‚ - context: {        â”‚\n",
    "            â”‚     last_plan       â”‚\n",
    "            â”‚   }                 â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â”‚\n",
    "                      v\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  EXECUTOR NODE (with RAG)                        â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚ 1. RETRIEVAL STEP                         â”‚  â”‚\n",
    "â”‚  â”‚    retriever.invoke(original_query)       â”‚  â”‚\n",
    "â”‚  â”‚           â†“                                â”‚  â”‚\n",
    "â”‚  â”‚    Vector Store (Chroma) with MMR         â”‚  â”‚\n",
    "â”‚  â”‚           â†“                                â”‚  â”‚\n",
    "â”‚  â”‚    Top-3 Relevant Documents               â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚ 2. SYNTHESIS STEP                         â”‚  â”‚\n",
    "â”‚  â”‚    INPUT:  context['last_plan']           â”‚  â”‚\n",
    "â”‚  â”‚            messages[0] (original query)   â”‚  â”‚\n",
    "â”‚  â”‚            retrieved_docs (from vector DB)â”‚  â”‚\n",
    "â”‚  â”‚    OUTPUT: \"Result: Attention mechanism   â”‚  â”‚\n",
    "â”‚  â”‚            allows models to focus on...   â”‚  â”‚\n",
    "â”‚  â”‚            [grounded in retrieved docs]\"  â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚  STORES: context['last_result'] = \"...\"         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â”‚\n",
    "                      v\n",
    "            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "            â”‚   STATE PASSED:     â”‚\n",
    "            â”‚ - messages: [       â”‚\n",
    "            â”‚     HumanMessage,   â”‚\n",
    "            â”‚     AIMessage,      â”‚\n",
    "            â”‚     AIMessage,      â”‚\n",
    "            â”‚     AIMessage]      â”‚\n",
    "            â”‚ - context: {        â”‚\n",
    "            â”‚     last_result     â”‚\n",
    "            â”‚   }                 â”‚\n",
    "            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â”‚\n",
    "                      v\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  EVALUATOR NODE                                  â”‚\n",
    "â”‚  INPUT:  context['last_result'] â† Reads result  â”‚\n",
    "â”‚  OUTPUT: \"Evaluation: SATISFACTORY\"              â”‚\n",
    "â”‚  ROUTES: \"end\" (or \"continue\" if needs work)    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â”‚\n",
    "                      v\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚   END NODE    â”‚\n",
    "              â”‚   (Complete)  â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "âœ… **Each node reads SPECIFIC data from previous nodes**  \n",
    "âœ… **Executor performs RAG: retrieval â†’ synthesis with plan**  \n",
    "âœ… **Context dict grows with each node** (last_analysis â†’ last_plan â†’ last_result)  \n",
    "âœ… **Messages list accumulates ALL outputs** (for conversation history)  \n",
    "âœ… **State is MERGED at each step** (not replaced)  \n",
    "âœ… **RAG enables grounded responses** from knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Workflow Architecture (with RAG)\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    User Query Input                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ANALYZER NODE                                               â”‚\n",
    "â”‚  â€¢ Identifies query intent                                   â”‚\n",
    "â”‚  â€¢ Extracts key requirements                                 â”‚\n",
    "â”‚  â€¢ System Prompt: \"Expert analyst\"                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  PLANNER NODE                                                â”‚\n",
    "â”‚  â€¢ Creates 2-3 step action plan                             â”‚\n",
    "â”‚  â€¢ Considers context from analyzer                           â”‚\n",
    "â”‚  â€¢ System Prompt: \"Strategic planner\"                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  EXECUTOR NODE (with RAG)                                    â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚  RAG STEP 1: Document Retrieval                     â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Query: Original user question                    â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Retriever: MMR search (k=3, fetch_k=10)          â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Source: Chroma vector store                      â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Result: Top-3 relevant documents                 â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚  RAG STEP 2: Synthesis & Generation                 â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Combines: Plan + Retrieved Docs + Original Query â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ Generates: Grounded, comprehensive answer        â”‚   â”‚\n",
    "â”‚  â”‚  â€¢ System Prompt: \"Knowledgeable assistant\"         â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  EVALUATOR NODE                                              â”‚\n",
    "â”‚  â€¢ Assesses response quality                                 â”‚\n",
    "â”‚  â€¢ Decides: SATISFACTORY or NEEDS_IMPROVEMENT               â”‚\n",
    "â”‚  â€¢ System Prompt: \"Quality assessor\"                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â”‚\n",
    "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”‚               â”‚\n",
    "                â–¼               â–¼\n",
    "        SATISFACTORY      NEEDS_IMPROVEMENT\n",
    "        (max iterations)  (error threshold)\n",
    "                â”‚               â”‚\n",
    "                â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                â”‚                          â”‚\n",
    "                â–¼                          â–¼\n",
    "            END NODE              Back to ANALYZER\n",
    "                                  (Iteration Loop)\n",
    "```\n",
    "\n",
    "### Key Features at Each Layer\n",
    "\n",
    "**ğŸ”„ Retry Policies**: Each node auto-retries up to 3 times on failure  \n",
    "**ğŸ’¾ Checkpointing**: State persisted after each node (thread-based)  \n",
    "**ğŸš€ Caching**: System prompts cached (128 max) for performance  \n",
    "**ğŸ“Š State Management**: Reducer functions handle message accumulation  \n",
    "**ğŸ“š RAG Integration**: Executor retrieves & synthesizes knowledge base documents  \n",
    "**ğŸ” Vector Store**: Chroma with HuggingFace embeddings (all-MiniLM-L6-v2)  \n",
    "**ğŸ¯ MMR Search**: Maximum Marginal Relevance for diverse results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Implementation Summary\n",
    "\n",
    "### Benefits Achieved:\n",
    "\n",
    "âœ… **OpenAI Compatibility**: Uses OpenAI-compatible message format  \n",
    "âœ… **Simpler Message Format**: No custom message classes needed  \n",
    "âœ… **Multi-Provider Support**: Access to 14+ inference providers  \n",
    "âœ… **Maintained Compatibility**: All retry policies, caching, and error handling intact  \n",
    "âœ… **Production Ready**: Tested and verified with complex multi-step workflows  \n",
    "âœ… **Streaming Support**: Native streaming capabilities available  \n",
    "âœ… **RAG Integration**: Retrieval-Augmented Generation with LangChain vector stores  \n",
    "âœ… **Grounded Responses**: Answers backed by knowledge base documents  \n",
    "âœ… **Reduced Hallucination**: Facts from vector store, not pure generation  \n",
    "âœ… **Flexible Architecture**: Works with or without retriever\n",
    "\n",
    "### RAG Components:\n",
    "\n",
    "ğŸ” **Vector Store**: Chroma with persistent storage  \n",
    "ğŸ§  **Embeddings**: HuggingFace all-MiniLM-L6-v2  \n",
    "ğŸ¯ **Search Strategy**: MMR (Maximum Marginal Relevance)  \n",
    "ğŸ“Š **Retrieval**: Top-3 documents with diversity balancing  \n",
    "ğŸ”— **Integration**: Seamless synthesis in Executor node\n",
    "\n",
    "### Next Steps (Optional):\n",
    "\n",
    "1. **Provider Selection**: Specify providers (hyperbolic, nebius, together) for better performance\n",
    "2. **Performance Tuning**: Optimize temperature and max_tokens for specific use cases\n",
    "3. **Multi-Model Support**: Add ability to switch between different HuggingFace models\n",
    "4. **Enhanced Caching**: Implement persistent caching for production deployment\n",
    "5. **RAG Optimization**: Tune retriever parameters (k, fetch_k, lambda_mult) for your domain\n",
    "6. **Custom Embeddings**: Experiment with domain-specific embedding models\n",
    "7. **Hybrid Search**: Combine semantic and keyword search for better retrieval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
