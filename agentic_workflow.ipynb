{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Workflow with HuggingFace InferenceClient + RAG\n",
    "\n",
    "Production-ready workflow with advanced features:\n",
    "- ‚úÖ Using HuggingFace InferenceClient for LLM calls that use `chat.completions.create()` method compatible with all HuggingFace Inference and OpenAI API\n",
    "- ‚úÖ RAG (Retrieval-Augmented Generation) with LangChain vector store retriever integration\n",
    "- ‚úÖ Retry policies with exponential backoff, error handling, and workflow logic\n",
    "- ‚úÖ Checkpointing for fault tolerance\n",
    "- ‚úÖ LLM caching for performance\n",
    "- ‚úÖ Structured logging and error handling\n",
    "- ‚úÖ Multi-node workflow (Analyzer ‚Üí Planner ‚Üí Executor with RAG ‚Üí Evaluator)\n",
    "- ‚úÖ Streaming support for real-time updates\n",
    "- ‚úÖ Chroma vector store for document retrieval with MMR search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ RAG-Enhanced AgenticWorkflow\n",
    "\n",
    "\n",
    "**Implementation Summary:**\n",
    "\n",
    "The `AgenticWorkflow` class configured to use HuggingFace InferenceClient with OpenAI-compatible API **and RAG (Retrieval-Augmented Generation)**.\n",
    "\n",
    "**Key features:**\n",
    "\n",
    "‚úÖ **API Compatibility:**\n",
    "- OpenAI-compatible message format\n",
    "- Compatible with all HuggingFace Inference API models\n",
    "- Multi-provider support (hyperbolic, nebius, together, etc.)\n",
    "- Native streaming capabilities\n",
    "\n",
    "‚ú® **Fully Integrated LangChain RAG**  \n",
    "- Analyzer ‚Üí Planner ‚Üí **Executor (with RAG)** ‚Üí Evaluator\n",
    "- Document retrieval from Chroma vector store\n",
    "- MMR search for relevant + diverse results\n",
    "- Automatic synthesis of retrieved context with LLM responses\n",
    "\n",
    "üìö **Knowledge Base**  \n",
    "- Vector store: Chroma (persistent)\n",
    "- Embeddings: all-MiniLM-L6-v2\n",
    "- Content: Transformer paper (Attention Is All You Need)\n",
    "- Search: Top-3 documents with MMR\n",
    "\n",
    "üîß **Production Features**  \n",
    "- Retry policies & error handling\n",
    "- State checkpointing & persistence\n",
    "- LLM caching for performance\n",
    "- Flexible: Works with or without RAG\n",
    "- Gradio UI with RAG enabled\n",
    "- Streaming support\n",
    "- Thread-based conversation tracking\n",
    "\n",
    "\n",
    "### Benefits of RAG Integration\n",
    "\n",
    "‚úÖ **Reduced Hallucination**: Responses grounded in actual documents  \n",
    "‚úÖ **Domain-Specific Knowledge**: Access to specialized information beyond LLM training  \n",
    "‚úÖ **Up-to-Date Information**: Retrieves from current document base  \n",
    "‚úÖ **Transparent Sourcing**: Can trace responses back to source documents  \n",
    "‚úÖ **Improved Accuracy**: Combines LLM reasoning with factual retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```\n",
    "User Query ‚Üí Analyzer ‚Üí Planner ‚Üí Executor (with RAG) ‚Üí Evaluator\n",
    "                                          ‚Üì\n",
    "                                    Retriever.invoke()\n",
    "                                          ‚Üì\n",
    "                                  Vector Store (Chroma)\n",
    "                                          ‚Üì\n",
    "                            Retrieved Documents (Top-3 MMR)\n",
    "                                          ‚Üì\n",
    "                            Synthesize with Plan + Query\n",
    "                                          ‚Üì\n",
    "                                    LLM Response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Quick Start Guide\n",
    "\n",
    "### Usage\n",
    "```python\n",
    "# Initialize retriever from your vector store\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# Create workflow with RAG enabled\n",
    "workflow_rag = AgenticWorkflow(\n",
    "    max_iterations=3,\n",
    "    retriever=retriever  # ‚Üê RAG enabled\n",
    ")\n",
    "\n",
    "# Responses will be grounded in your knowledge base\n",
    "response = workflow_rag.get_response(\"Question about your documents\")\n",
    "```\n",
    "\n",
    "### Available Parameters\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "|-----------|------|---------|-------------|\n",
    "| `model_id` | str | `meta-llama/Llama-3.1-8B-Instruct` | HuggingFace model to use |\n",
    "| `max_iterations` | int | `5` | Max refinement cycles (1-10 recommended) |\n",
    "| `temperature` | float | `0.5` | Creativity (0.0=deterministic, 1.0=creative) |\n",
    "| `max_new_tokens` | int | `512` | Max response length (256-2048) |\n",
    "| `enable_checkpointing` | bool | `True` | Enable state persistence |\n",
    "| `retry_on_error` | bool | `True` | Auto-retry failed nodes |\n",
    "| `retriever` | Optional[Any] | `None` | **LangChain retriever for RAG** |\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "1. **Update file paths**: Set your PDF file path in `pdf_path` and Chroma database directory in `persist_directory`\n",
    "2. **Run the test cells** to validate RAG integration\n",
    "3. **Adjust retriever parameters** (`k`, `fetch_k`, `lambda_mult`) based on results\n",
    "4. **Experiment with search types**: Try `\"similarity\"` or `\"similarity_score_threshold\"`\n",
    "5. **Monitor performance**: Track retrieval latency and response quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Dependencies\n",
    "\n",
    "Loading environment variables and configuring LangChain/LangGraph infrastructure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Environment and caching configured\n",
      "‚úÖ Huggingface Inference client configured\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Environment and caching configured\n",
      "‚úÖ Huggingface Inference client configured\n"
     ]
    }
   ],
   "source": [
    "# Using InferenceClient with chat.completions API (compatible with more providers)\n",
    "%pip install -q -U huggingface-hub langchain-huggingface python-dotenv gradio langgraph langchain-core langchain-community langchain-chroma sentence-transformers\n",
    "\n",
    "# Install the langchain and pypdf packages\n",
    "%pip -q install -U pypdf langchain-chroma langchain-community langchain-text-splitters\n",
    "# Import required modules\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "from typing import TypedDict, Annotated, List, Optional, Dict, Any\n",
    "from functools import lru_cache\n",
    "load_dotenv()\n",
    "sys.path.append('..')\n",
    "from huggingface_hub import InferenceClient\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.types import RetryPolicy\n",
    "from langchain_community.cache import InMemoryCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import gradio as gr\n",
    "print('‚úÖ Environment and caching configured')\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "client = InferenceClient(token=hf_token)\n",
    "llama_model = \"meta-llama/Llama-3.1-8B-Instruct\"  \n",
    "print('‚úÖ Huggingface Inference client configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Vector Store Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store retriever initialized\n",
      "   ‚îî‚îÄ Search type: MMR\n",
      "   ‚îî‚îÄ Top-k results: 3\n",
      "   ‚îî‚îÄ Embedding model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "pdf_path = r\"Your path\"\n",
    "\n",
    "# Load the PDF document\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)  # Adjust overlap as needed\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize the all-MiniLM-L6-v2 embedding model\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create a Chroma vector store and embed the chunks\n",
    "vector_store = Chroma.from_documents(\n",
    "    split_documents, \n",
    "    embedding=embedding_function,\n",
    "    persist_directory=\"Your Directory\"\n",
    ")\n",
    "\n",
    "# Create a retriever with optimized search parameters\n",
    "# Using MMR (Maximum Marginal Relevance) for diverse results\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",  # Maximum Marginal Relevance for diverse results\n",
    "    search_kwargs={\n",
    "        \"k\": 3,  # Retrieve top 3 most relevant documents\n",
    "        \"fetch_k\": 10,  # Fetch 10 candidates before MMR filtering\n",
    "        \"lambda_mult\": 0.7  # Balance between relevance (1.0) and diversity (0.0)\n",
    "    }\n",
    ")\n",
    "\n",
    "print('‚úÖ Vector store retriever initialized')\n",
    "print(f'   ‚îî‚îÄ Search type: MMR')\n",
    "print(f'   ‚îî‚îÄ Top-k results: 3')\n",
    "print(f'   ‚îî‚îÄ Embedding model: all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent State with Reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced state defined\n"
     ]
    }
   ],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    state of the agent workflow.\n",
    "\n",
    "    Uses annotated types with reducer functions for proper state management.\n",
    "    \"\"\"\n",
    "    # Use add_messages reducer for proper message handling\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    next_action: Annotated[str, operator.add]\n",
    "    iterations: Annotated[int, operator.add]\n",
    "    context: Annotated[Dict[str, Any], lambda x,y: {**x, **y}]\n",
    "    error_count: Annotated[int, operator.add]\n",
    "\n",
    "print('‚úÖ Enhanced state defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticWorkflow:\n",
    "    \"\"\"\n",
    "    Agentic workflow with best practices from LangGraph and LangChain.\n",
    "    Components:\n",
    "    - State management with proper reducer functions\n",
    "    - Error handling with retry policies\n",
    "    - Performance optimization through caching\n",
    "    - Checkpointing for fault tolerance\n",
    "    - Efficient prompt engineering\n",
    "    - Parallel execution support\n",
    "    - HuggingFace InferenceClient integration (OpenAI-compatible API)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str = llama_model,\n",
    "        max_iterations: int = 5,\n",
    "        temperature: float = 0.5,\n",
    "        max_new_tokens: int = 1024,\n",
    "        enable_checkpointing: bool = True,\n",
    "        retry_on_error: bool = True,\n",
    "        retriever: Optional[Any] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the agentic workflow.\n",
    "        \n",
    "        Args:\n",
    "            model_id: HuggingFace model identifier (e.g., \"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "            max_iterations: Maximum number of workflow iterations\n",
    "            temperature: LLM temperature for response variability\n",
    "            max_new_tokens: Maximum tokens to generate\n",
    "            enable_checkpointing: Enable state persistence\n",
    "            retry_on_error: Enable automatic retries on failures\n",
    "            retriever: LangChain retriever for RAG (Retrieval-Augmented Generation)\n",
    "        \"\"\"\n",
    "        self.max_iterations = max_iterations\n",
    "        self.retry_on_error = retry_on_error\n",
    "        self.model_id = model_id\n",
    "        self.temperature = temperature\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.retriever = retriever\n",
    "        \n",
    "        # Initialize HuggingFace InferenceClient with optimized settings\n",
    "        try:\n",
    "            self.llm = client\n",
    "            print(f\"‚úÖ Initialized HuggingFace InferenceClient with model: {model_id}\")\n",
    "            if self.retriever:\n",
    "                print(f\"‚úÖ RAG enabled with retriever integration\")\n",
    "        except Exception as e:\n",
    "            raise\n",
    "        \n",
    "        # Initialize checkpointer for state persistence\n",
    "        self.checkpointer = InMemorySaver() if enable_checkpointing else None\n",
    "        \n",
    "        # Build the workflow graph\n",
    "        self.graph = self._build_graph()\n",
    "        print(\"‚úÖ Workflow graph compiled successfully\")\n",
    "    \n",
    "    def _build_graph(self):\n",
    "        \"\"\"\n",
    "        Build the enhanced LangGraph workflow with retry policies.\n",
    "        \n",
    "        Returns:\n",
    "            Compiled StateGraph instance with checkpointing\n",
    "        \"\"\"\n",
    "        # Create workflow graph with enhanced state schema\n",
    "        workflow = StateGraph(state_schema=AgentState)\n",
    "        \n",
    "        # Define retry policy for nodes\n",
    "        retry_policy = RetryPolicy(\n",
    "            max_attempts=3,\n",
    "            retry_on=Exception  # Retry on any exception\n",
    "        ) if self.retry_on_error else None\n",
    "        \n",
    "        # Add nodes with retry policies\n",
    "        workflow.add_node(\n",
    "            \"analyzer\", \n",
    "            self._analyze_query,\n",
    "            retry_policy=retry_policy\n",
    "        )\n",
    "        workflow.add_node(\n",
    "            \"planner\", \n",
    "            self._plan_response,\n",
    "            retry_policy=retry_policy\n",
    "        )\n",
    "        workflow.add_node(\n",
    "            \"executor\", \n",
    "            self._execute_plan,\n",
    "            retry_policy=retry_policy\n",
    "        )\n",
    "        workflow.add_node(\n",
    "            \"evaluator\", \n",
    "            self._evaluate_result,\n",
    "            retry_policy=retry_policy\n",
    "        )\n",
    "        \n",
    "        # Define entry point\n",
    "        workflow.set_entry_point(\"analyzer\")\n",
    "        \n",
    "        # Add edges\n",
    "        workflow.add_edge(\"analyzer\", \"planner\")\n",
    "        workflow.add_edge(\"planner\", \"executor\")\n",
    "        workflow.add_edge(\"executor\", \"evaluator\")\n",
    "        \n",
    "        # Add conditional routing from evaluator\n",
    "        workflow.add_conditional_edges(\n",
    "            \"evaluator\",\n",
    "            self._should_continue,\n",
    "            {\n",
    "                \"continue\": \"analyzer\",\n",
    "                \"end\": END\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Compile with checkpointing\n",
    "        return workflow.compile(checkpointer=self.checkpointer)\n",
    "    \n",
    "    @lru_cache(maxsize=128)\n",
    "    def _get_system_prompt(self, task: str) -> str:\n",
    "        \"\"\"\n",
    "        Get cached system prompts for different tasks.\n",
    "        \n",
    "        Args:\n",
    "            task: The task type (analyze, plan, execute, evaluate)\n",
    "            \n",
    "        Returns:\n",
    "            System prompt for the task\n",
    "        \"\"\"\n",
    "        prompts = {\n",
    "            \"analyze\": \"You are an expert analyst. Carefully examine queries and identify core intents concisely.\",\n",
    "            \"plan\": \"You are a strategic planner. Create clear, actionable step-by-step plans.\",\n",
    "            \"execute\": \"You are a knowledgeable assistant. Provide comprehensive, accurate answers.\",\n",
    "            \"evaluate\": \"You are a quality assessor. Evaluate responses objectively and concisely.\"\n",
    "        }\n",
    "        return prompts.get(task, \"You are a helpful AI assistant.\")\n",
    "    \n",
    "    def _handle_llm_call(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        system_prompt: Optional[str] = None,\n",
    "        max_retries: int = 3\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Robust LLM call with error handling and retries.\n",
    "        Args:\n",
    "            prompt: The user prompt\n",
    "            system_prompt: Optional system prompt\n",
    "            max_retries: Maximum number of retry attempts\n",
    "            \n",
    "        Returns:\n",
    "            LLM response text\n",
    "            \n",
    "        Raises:\n",
    "            Exception: If all retries fail\n",
    "        \"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Construct messages for HuggingFace InferenceClient (OpenAI-compatible format)\n",
    "                messages = []\n",
    "                \n",
    "                # Add system message if provided\n",
    "                if system_prompt:\n",
    "                    messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "                \n",
    "                # Add user message\n",
    "                messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "                \n",
    "                # Call HuggingFace InferenceClient's chat.completions.create() method\n",
    "                response = self.llm.chat.completions.create(\n",
    "                    model=self.model_id,\n",
    "                    messages=messages,\n",
    "                    temperature=self.temperature,\n",
    "                    max_tokens=self.max_new_tokens\n",
    "                )\n",
    "                \n",
    "                # Extract content from response\n",
    "                return response.choices[0].message.content\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  LLM call attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(f\"‚ùå All {max_retries} LLM call attempts failed\")\n",
    "                    raise\n",
    "        \n",
    "        return \"Error: Failed to get LLM response\"\n",
    "    \n",
    "    def _analyze_query(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze the user query to understand intent with improved prompting.\n",
    "        \n",
    "        Args:\n",
    "            state: Current agent state\n",
    "            \n",
    "        Returns:\n",
    "            State update dict\n",
    "        \"\"\"\n",
    "        try:\n",
    "            messages = state[\"messages\"]\n",
    "            last_message = messages[-1].content if messages else \"\"\n",
    "            \n",
    "            # Optimized prompt engineering\n",
    "            system_prompt = self._get_system_prompt(\"analyze\")\n",
    "            prompt = f\"\"\"Query: {last_message}\n",
    "\n",
    "Task: Identify the main intent in 1-2 sentences. Focus on what the user wants to accomplish.\n",
    "\n",
    "Analysis:\"\"\"\n",
    "            \n",
    "            response = self._handle_llm_call(prompt, system_prompt)\n",
    "            \n",
    "            # Return state update\n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Analysis: {response}\")],\n",
    "                \"next_action\": \"plan\",\n",
    "                \"context\": {\"last_analysis\": response}\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Analysis failed: {e}\")\n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Analysis Error: {str(e)}\")],\n",
    "                \"next_action\": \"end\",\n",
    "                \"error_count\": 1\n",
    "            }\n",
    "    \n",
    "    def _plan_response(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Plan the response based on the analysis with concise prompting.\n",
    "        \n",
    "        Args:\n",
    "            state: Current agent state\n",
    "            \n",
    "        Returns:\n",
    "            State update dict\n",
    "        \"\"\"\n",
    "        try:\n",
    "            messages = state[\"messages\"]\n",
    "            \n",
    "            # Get only relevant context (last 3 messages)\n",
    "            context = \"\\n\".join([str(m.content) for m in messages[-3:]])\n",
    "            \n",
    "            system_prompt = self._get_system_prompt(\"plan\")\n",
    "            prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Task: Create a brief 2-3 step plan to address the user's request.\n",
    "\n",
    "Plan:\"\"\"\n",
    "            \n",
    "            response = self._handle_llm_call(prompt, system_prompt)\n",
    "            \n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Plan: {response}\")],\n",
    "                \"next_action\": \"execute\",\n",
    "                \"context\": {\"last_plan\": response}\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Planning failed: {e}\")\n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Planning Error: {str(e)}\")],\n",
    "                \"next_action\": \"end\",\n",
    "                \"error_count\": 1\n",
    "            }\n",
    "    \n",
    "    def _execute_plan(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the planned response with RAG (Retrieval-Augmented Generation).\n",
    "        \n",
    "        Args:\n",
    "            state: Current agent state\n",
    "            \n",
    "        Returns:\n",
    "            State update dict\n",
    "        \"\"\"\n",
    "        try:\n",
    "            messages = state[\"messages\"]\n",
    "            original_query = messages[0].content\n",
    "            \n",
    "            # Use context from state if available\n",
    "            plan = state.get(\"context\", {}).get(\"last_plan\", \"\")\n",
    "            if not plan:\n",
    "                plan = messages[-1].content if messages else \"\"\n",
    "            \n",
    "            # RAG Integration: Retrieve relevant documents if retriever is available\n",
    "            context_text = \"\"\n",
    "            if self.retriever:\n",
    "                try:\n",
    "                    # Retrieve relevant documents from vector store\n",
    "                    retrieved_docs = self.retriever.invoke(original_query)\n",
    "                    \n",
    "                    # Format retrieved documents as context\n",
    "                    if retrieved_docs:\n",
    "                        context_text = \"\\n\\n\".join([\n",
    "                            f\"Document {i+1}:\\n{doc.page_content}\" \n",
    "                            for i, doc in enumerate(retrieved_docs)\n",
    "                        ])\n",
    "                        print(f\"üìö Retrieved {len(retrieved_docs)} relevant documents from vector store\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Retrieval failed: {e}. Proceeding without RAG.\")\n",
    "                    context_text = \"\"\n",
    "            \n",
    "            system_prompt = self._get_system_prompt(\"execute\")\n",
    "            \n",
    "            # Build prompt with or without retrieved context\n",
    "            if context_text:\n",
    "                prompt = f\"\"\"Original Query: {original_query}\n",
    "\n",
    "Plan: {plan}\n",
    "\n",
    "Retrieved Context from Knowledge Base:\n",
    "{context_text}\n",
    "\n",
    "Task: Synthesize the plan with the retrieved context to provide a comprehensive, accurate answer to the original query. Use the retrieved documents to support your response with specific information.\n",
    "\n",
    "Answer:\"\"\"\n",
    "            else:\n",
    "                prompt = f\"\"\"Original Query: {original_query}\n",
    "\n",
    "Plan: {plan}\n",
    "\n",
    "Task: Provide a comprehensive, accurate answer to the original query.\n",
    "\n",
    "Answer:\"\"\"\n",
    "            \n",
    "            response = self._handle_llm_call(prompt, system_prompt)\n",
    "            \n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Result: {response}\")],\n",
    "                \"next_action\": \"evaluate\",\n",
    "                \"context\": {\"last_result\": response}\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Execution failed: {e}\")\n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Execution Error: {str(e)}\")],\n",
    "                \"next_action\": \"end\",\n",
    "                \"error_count\": 1\n",
    "            }\n",
    "    \n",
    "    def _evaluate_result(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate if the result is satisfactory with improved criteria.\n",
    "        \n",
    "        Args:\n",
    "            state: Current agent state\n",
    "            \n",
    "        Returns:\n",
    "            State update dict\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Increment iterations\n",
    "            current_iterations = state.get(\"iterations\", 0) + 1\n",
    "            \n",
    "            messages = state[\"messages\"]\n",
    "            result = state.get(\"context\", {}).get(\"last_result\", \"\")\n",
    "            if not result:\n",
    "                result = messages[-1].content if messages else \"\"\n",
    "            \n",
    "            system_prompt = self._get_system_prompt(\"evaluate\")\n",
    "            prompt = f\"\"\"Result: {result}\n",
    "\n",
    "Task: Evaluate if this result is satisfactory. Respond ONLY with:\n",
    "- 'SATISFACTORY' if the answer is complete and accurate\n",
    "- 'NEEDS_IMPROVEMENT' if it requires refinement\n",
    "\n",
    "Evaluation:\"\"\"\n",
    "            \n",
    "            response = self._handle_llm_call(prompt, system_prompt)\n",
    "            \n",
    "            # Determine next action\n",
    "            if \"SATISFACTORY\" in response.upper() or current_iterations >= self.max_iterations:\n",
    "                next_action = \"end\"\n",
    "            else:\n",
    "                next_action = \"continue\"\n",
    "            \n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Evaluation: {response}\")],\n",
    "                \"next_action\": next_action,\n",
    "                \"iterations\": 1,  # Increment by 1 (operator.add)\n",
    "                \"context\": {\"last_evaluation\": response}\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Evaluation failed: {e}\")\n",
    "            return {\n",
    "                \"messages\": [AIMessage(content=f\"Evaluation Error: {str(e)}\")],\n",
    "                \"next_action\": \"end\",\n",
    "                \"iterations\": 1,\n",
    "                \"error_count\": 1\n",
    "            }\n",
    "    from typing import Literal\n",
    "    def _should_continue(self, state: AgentState) -> Literal[\"continue\", \"end\"]:\n",
    "        \"\"\"\n",
    "        Decide whether to continue or end the workflow with enhanced logic.\n",
    "        \n",
    "        Args:\n",
    "            state: Current agent state\n",
    "            \n",
    "        Returns:\n",
    "            'continue' or 'end'\n",
    "        \"\"\"\n",
    "        # Check error threshold\n",
    "        if state.get(\"error_count\", 0) >= 2:\n",
    "            print(\"‚ö†Ô∏è  Too many errors, ending workflow\")\n",
    "            return \"end\"\n",
    "        \n",
    "        # Check iteration limit\n",
    "        if state.get(\"iterations\", 0) >= self.max_iterations:\n",
    "            print(f\"‚ÑπÔ∏è  Reached max iterations ({self.max_iterations})\")\n",
    "            return \"end\"\n",
    "        \n",
    "        # Check next_action\n",
    "        next_action = state.get(\"next_action\", \"end\")\n",
    "        if next_action == \"continue\":\n",
    "            print(\"üîÑ Continuing workflow for improvement\")\n",
    "            return \"continue\"\n",
    "        \n",
    "        print(\"‚úÖ Workflow complete\")\n",
    "        return \"end\"\n",
    "    \n",
    "    def run(\n",
    "        self, \n",
    "        query: str, \n",
    "        thread_id: Optional[str] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run the agentic workflow with a user query.\n",
    "        \n",
    "        Args:\n",
    "            query: User input query\n",
    "            thread_id: Optional thread ID for checkpointing\n",
    "            \n",
    "        Returns:\n",
    "            Final state of the workflow\n",
    "        \"\"\"\n",
    "        # Initial state\n",
    "        initial_state: AgentState = {\n",
    "            \"messages\": [HumanMessage(content=query)],\n",
    "            \"next_action\": \"analyze\",\n",
    "            \"iterations\": 0,\n",
    "            \"context\": {},\n",
    "            \"error_count\": 0\n",
    "        }\n",
    "        \n",
    "        # Configuration for checkpointing\n",
    "        config = {\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": thread_id or \"default\"\n",
    "            }\n",
    "        } if self.checkpointer else {}\n",
    "        \n",
    "        try:\n",
    "            print(f\"üöÄ Starting workflow for query: {query[:50]}...\")\n",
    "            final_state = self.graph.invoke(initial_state, config=config)\n",
    "            print(\"‚úÖ Workflow completed successfully\")\n",
    "            return final_state\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Workflow failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def stream(\n",
    "        self, \n",
    "        query: str, \n",
    "        thread_id: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Stream the workflow execution for real-time updates.\n",
    "        \n",
    "        Args:\n",
    "            query: User input query\n",
    "            thread_id: Optional thread ID for checkpointing\n",
    "            \n",
    "        Yields:\n",
    "            State updates as they occur\n",
    "        \"\"\"\n",
    "        initial_state: AgentState = {\n",
    "            \"messages\": [HumanMessage(content=query)],\n",
    "            \"next_action\": \"analyze\",\n",
    "            \"iterations\": 0,\n",
    "            \"context\": {},\n",
    "            \"error_count\": 0\n",
    "        }\n",
    "        \n",
    "        config = {\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": thread_id or \"default\"\n",
    "            }\n",
    "        } if self.checkpointer else {}\n",
    "        \n",
    "        try:\n",
    "            for chunk in self.graph.stream(initial_state, config=config, stream_mode=\"updates\"):\n",
    "                yield chunk\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Streaming failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def get_response(self, query: str, thread_id: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Get a simple text response from the workflow.\n",
    "        \n",
    "        Args:\n",
    "            query: User input query\n",
    "            thread_id: Optional thread ID for checkpointing\n",
    "            \n",
    "        Returns:\n",
    "            Final response text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self.run(query, thread_id)\n",
    "            \n",
    "            # Extract the final meaningful response\n",
    "            messages = result.get(\"messages\", [])\n",
    "            for msg in reversed(messages):\n",
    "                if isinstance(msg, AIMessage) and msg.content.startswith(\"Result:\"):\n",
    "                    return msg.content.replace(\"Result:\", \"\").strip()\n",
    "            \n",
    "            return \"No response generated.\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to get response: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test RAG-Enabled Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üß™ Testing RAG-Enabled Agentic Workflow\n",
      "================================================================================\n",
      "‚úÖ Initialized HuggingFace InferenceClient with model: meta-llama/Llama-3.1-8B-Instruct\n",
      "‚úÖ RAG enabled with retriever integration\n",
      "‚úÖ Workflow graph compiled successfully\n",
      "\n",
      "üìù Query: What is the attention mechanism in transformers?\n",
      "\n",
      "üîÑ Running workflow...\n",
      "\n",
      "üöÄ Starting workflow for query: What is the attention mechanism in transformers?...\n",
      "üìö Retrieved 3 relevant documents from vector store\n",
      "‚úÖ Workflow complete\n",
      "‚úÖ Workflow completed successfully\n",
      "\n",
      "================================================================================\n",
      "‚úÖ FINAL RESPONSE:\n",
      "================================================================================\n",
      "**What is the attention mechanism in transformers?**\n",
      "\n",
      "**Step 1: Define the Basics of Attention Mechanism**\n",
      "\n",
      "The attention mechanism is a technique used in deep learning to help models focus on the most relevant parts of the input data. In the context of transformers, self-attention is used to allow the model to weigh the importance of different input elements relative to each other. This mechanism enables the model to attend to specific parts of the input sequence and weigh their importance, rather than considering all parts equally.\n",
      "\n",
      "**Example**: \"In the self-attention mechanism, the model computes the query, key, and value vectors (Q, K, V) from the input embedding. It then computes the dot product of Q and K to get the attention weights, which are then passed through a softmax function to normalize the weights. The final output is a weighted sum of the value vectors, based on the attention weights.\"\n",
      "\n",
      "**Step 2: Explain the Math Behind Self-Attention**\n",
      "\n",
      "In self-attention, we compute the query, key, and value vectors (Q, K, V) from the input embedding. We then compute the dot product of Q and K to get the attention weights, which are then passed through a softmax function to normalize the weights. The final output is a weighted sum of the value vectors, based on the attention weights.\n",
      "\n",
      "Mathematically, the self-attention mechanism can be represented as follows:\n",
      "\n",
      "* Query (Q), Key (K), and Value (V) vectors: `Q = W_Q * x`, `K = W_K * x`, `V = W_V * x`\n",
      "* Dot product attention: `attention_weights = softmax(Q * K^T / sqrt(d))`\n",
      "* Softmax function: `softmax(x) = exp(x) / sum(exp(x))`\n",
      "* Weighted sum of value vectors: `output = attention_weights * V`\n",
      "\n",
      "where `x` is the input embedding, `W_Q`, `W_K`, and `W_V` are learnable weight matrices, and `d` is the dimensionality of the input embedding.\n",
      "\n",
      "**Step 3: Visualize and Apply the Attention Mechanism**\n",
      "\n",
      "The self-attention mechanism can be visualized as follows:\n",
      "\n",
      "Input Embedding ‚Üí Query, Key, Value Vectors ‚Üí Dot Product Attention ‚Üí Softmax ‚Üí Weighted Sum of Value Vectors\n",
      "\n",
      "The attention mechanism can be applied in various real-world scenarios, such as text classification or machine translation. For example, in text classification tasks, the model can focus on specific words or phrases that are relevant to the sentiment. In machine translation, the attention mechanism allows the model to focus on specific words or phrases in the source language that correspond to specific words or phrases in the target language.\n",
      "\n",
      "**Document 1 Support**: The Transformer uses multi-head attention in three different ways: encoder-decoder attention, self-attention in the encoder, and self-attention in the decoder. In self-attention layers, all of the keys, values, and queries come from the same place, allowing each position in the encoder or decoder to attend to all positions in the previous layer.\n",
      "\n",
      "**Document 2 Support**: An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the value vectors, based on the attention weights.\n",
      "\n",
      "**Document 3 Support**: Figure 3 illustrates an example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‚Äòmaking‚Äô, completing the phrase ‚Äòmaking...more difficult‚Äô. Attentions are shown only for the word ‚Äòmaking‚Äô. Different colors represent different heads.\n",
      "\n",
      "In conclusion, the attention mechanism in transformers is a technique used to focus on specific parts of the input data, weighing their importance relative to each other. The self-attention mechanism allows the model to attend to all positions in the input sequence, enabling it to capture long-distance dependencies and perform tasks such as text classification and machine translation.\n"
     ]
    }
   ],
   "source": [
    "# Test AgenticWorkflow \n",
    "print(\"=\" * 80)\n",
    "print(\"üß™ Testing RAG-Enabled Agentic Workflow\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize workflow with retriever\n",
    "workflow_rag = AgenticWorkflow(\n",
    "    max_iterations=3,\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=1024,\n",
    "    enable_checkpointing=True,\n",
    "    retry_on_error=True,\n",
    "    retriever=retriever  \n",
    ")\n",
    "\n",
    "# Test query \n",
    "test_query = \"What is the attention mechanism in transformers?\"\n",
    "\n",
    "print(f\"\\nüìù Query: {test_query}\\n\")\n",
    "print(\"üîÑ Running workflow...\\n\")\n",
    "\n",
    "response = workflow_rag.get_response(\n",
    "    test_query,\n",
    "    thread_id=\"rag_test_1\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ FINAL RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Workflow with custom queries using Gradio user interface (RAG-Enabled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Initialized HuggingFace InferenceClient with model: meta-llama/Llama-3.1-8B-Instruct\n",
      "‚úÖ RAG enabled with retriever integration\n",
      "‚úÖ Workflow graph compiled successfully\n",
      "üöÄ Starting workflow for query: do you have access to any document?...\n",
      "üìö Retrieved 3 relevant documents from vector store\n",
      "‚úÖ Workflow complete\n",
      "‚úÖ Workflow completed successfully\n",
      "‚úÖ Initialized HuggingFace InferenceClient with model: meta-llama/Llama-3.1-8B-Instruct\n",
      "‚úÖ RAG enabled with retriever integration\n",
      "‚úÖ Workflow graph compiled successfully\n",
      "üöÄ Starting workflow for query: do you remember my previous question?...\n",
      "üìö Retrieved 3 relevant documents from vector store\n",
      "‚úÖ Workflow complete\n",
      "‚úÖ Workflow completed successfully\n"
     ]
    }
   ],
   "source": [
    "def chat_with_workflow(message: str, history: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Chat function that integrates AgenticWorkflow with Gradio.\n",
    "    RAG-enabled by default to provide grounded responses.\n",
    "    \n",
    "    Args:\n",
    "        message: User's input message\n",
    "        history: Conversation history \n",
    "        \n",
    "    Returns:\n",
    "        Final response from the workflow\n",
    "    \"\"\"\n",
    "    # Create workflow instance\n",
    "    workflow = AgenticWorkflow(\n",
    "        max_iterations=3,\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=1024,\n",
    "        enable_checkpointing=True,\n",
    "        retry_on_error=True,\n",
    "        retriever=retriever \n",
    "    )\n",
    "    \n",
    "    # Get response using the workflow\n",
    "    try:\n",
    "        response = workflow.get_response(message, thread_id=\"gradio_session\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error processing request: {str(e)}\"\n",
    "\n",
    "# Create Gradio chat interface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_with_workflow,\n",
    "    type=\"messages\",\n",
    "    title=\"Agentic RAG Workflow Chat\",\n",
    "    description=\"Chat with an AI agent that analyzes, plans, executes with RAG, and evaluates responses. Responses are grounded in the knowledge base (transformer paper).\",\n",
    "    examples=[\n",
    "        \"Explain how Transformers differs from previous architectures?\",\n",
    "        \"What is the attention mechanism in transformers?\",\n",
    "        \"Explain the key innovations in the attention is all you need paper\",\n",
    "        \"What are the key components of a Transformer model?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Verify Inter-Node Communication\n",
    "\n",
    "**What This Test Does:**\n",
    "\n",
    "This test explicitly shows the **actual data flow** in a clean, single-pass format:\n",
    "how nodes pass information to each other through the state.\n",
    "\n",
    "‚úÖ **INPUT**: What specific data each node reads  \n",
    "‚úÖ **OUTPUT**: What each node produces  \n",
    "‚úÖ **STORES**: What gets saved in `context` for the next node  \n",
    "‚úÖ **ROUTES**: Where the workflow goes next\n",
    "\n",
    "**Key Points:**\n",
    "- Shows the **exact content** being passed between nodes\n",
    "- Tracks how `context` dict grows as state flows through the workflow\n",
    "- Makes it crystal clear that nodes read **specific parts** of the state\n",
    "\n",
    "**Run the test below to see the clean, simplified data flow!** üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç INTER-NODE COMMUNICATION TEST\n",
      "================================================================================\n",
      "\n",
      "This test shows EXACTLY what each node receives and produces.\n",
      "\n",
      "‚úÖ Initialized HuggingFace InferenceClient with model: meta-llama/Llama-3.1-8B-Instruct\n",
      "‚úÖ Workflow graph compiled successfully\n",
      "üìù Original Query: 'What is machine learning?'\n",
      "\n",
      "================================================================================\n",
      "STEP 1: ANALYZER\n",
      "================================================================================\n",
      "1.ANALYZER reads user query ‚Üí produces analysis ‚Üí stores in context\n",
      "üì• INPUT:  'What is machine learning?'\n",
      "üì§ OUTPUT: 'Analysis: The main intent of the user is to gain a fundamental underst...'\n",
      "üíæ STORES: context['last_analysis'] = 'The main intent of the user is to gain a fundamental understanding of ...'\n",
      "üîÄ ROUTES: ‚Üí plan\n",
      "\n",
      "================================================================================\n",
      "STEP 2: PLANNER\n",
      "================================================================================\n",
      "2. PLANNER reads analysis from messages[-3:] ‚Üí produces plan ‚Üí stores in context\n",
      "üì• INPUT:  messages[-3:] contains Analyzer's: 'Analysis: The main intent of the user is to gain a fundamental underst...'\n",
      "üì§ OUTPUT: 'Plan: **Step 1: Define Machine Learning**\n",
      "- Provide a clear and concis...'\n",
      "üíæ STORES: context['last_plan'] = '**Step 1: Define Machine Learning**\n",
      "- Provide a clear and concise defi...'\n",
      "üîÄ ROUTES: ‚Üí execute\n",
      "\n",
      "================================================================================\n",
      "STEP 3: EXECUTOR\n",
      "================================================================================\n",
      "3. EXECUTOR reads plan from context + original query ‚Üí produces result ‚Üí stores in context\n",
      "üì• INPUT:  context['last_plan'] = '**Step 1: Define Machine Learning**\n",
      "- Provide a clear and concise defi...'\n",
      "           messages[0] = 'What is machine learning?'\n",
      "üì§ OUTPUT: 'Result: **What is Machine Learning?**\n",
      "\n",
      "Machine learning is a subset of...'\n",
      "üíæ STORES: context['last_result'] = '**What is Machine Learning?**\n",
      "\n",
      "Machine learning is a subset of artific...'\n",
      "üîÄ ROUTES: ‚Üí evaluate\n",
      "\n",
      "‚ÑπÔ∏è  Reached max iterations (1)\n",
      "================================================================================\n",
      "STEP 4: EVALUATOR\n",
      "================================================================================\n",
      "   4. EVALUATOR reads result from context ‚Üí produces evaluation ‚Üí routes to end\n",
      "üì• INPUT:  context['last_result'] = '**What is Machine Learning?**\n",
      "\n",
      "Machine learning is a subset of artific...'\n",
      "üì§ OUTPUT: 'Evaluation: SATISFACTORY...'\n",
      "üíæ STORES: context['last_evaluation'] = 'SATISFACTORY...'\n",
      "üîÄ ROUTES: ‚Üí end\n",
      "\n",
      "================================================================================\n",
      "‚úÖ COMMUNICATION FLOW VERIFIED\n",
      "================================================================================\n",
      "\n",
      "üîó Summary:\n",
      "   1. ANALYZER reads user query ‚Üí produces analysis ‚Üí stores in context\n",
      "   2. PLANNER reads analysis from messages[-3:] ‚Üí produces plan ‚Üí stores in context\n",
      "   3. EXECUTOR reads plan from context + original query ‚Üí produces result ‚Üí stores in context\n",
      "   4. EVALUATOR reads result from context ‚Üí produces evaluation ‚Üí routes to end\n",
      "\n",
      "‚ú® Each node explicitly uses the previous node's output!\n"
     ]
    }
   ],
   "source": [
    "# üî¨ Deep Inspection: Track How Nodes Communicate\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç INTER-NODE COMMUNICATION TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nThis test shows EXACTLY what each node receives and produces.\\n\")\n",
    "\n",
    "# Create workflow with verbose tracking\n",
    "workflow_debug = AgenticWorkflow(\n",
    "    max_iterations=1,  # Single pass to see clear communication\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=256\n",
    ")\n",
    "\n",
    "test_query = \"What is machine learning?\"\n",
    "print(f\"üìù Original Query: '{test_query}'\\n\")\n",
    "\n",
    "# Track cumulative state to show what gets passed between nodes\n",
    "cumulative_state = {\"messages\": [], \"context\": {}}\n",
    "step_num = 0\n",
    "\n",
    "for update in workflow_debug.stream(test_query, thread_id=\"comm_test\"):\n",
    "    step_num += 1\n",
    "    node_name = list(update.keys())[0]\n",
    "    node_output = update[node_name]\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"STEP {step_num}: {node_name.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Show what this node READS\n",
    "    if node_name == \"analyzer\":\n",
    "        print(\"1.ANALYZER reads user query ‚Üí produces analysis ‚Üí stores in context\")\n",
    "        print(f\"üì• INPUT:  '{test_query}'\")\n",
    "\n",
    "        \n",
    "    elif node_name == \"planner\":\n",
    "        # Planner reads last 3 messages\n",
    "        print(\"2. PLANNER reads analysis from messages[-3:] ‚Üí produces plan ‚Üí stores in context\")\n",
    "        analyzer_msg = cumulative_state[\"messages\"][-1].content if cumulative_state[\"messages\"] else \"\"\n",
    "        print(f\"üì• INPUT:  messages[-3:] contains Analyzer's: '{analyzer_msg[:70]}...'\")\n",
    "\n",
    "        \n",
    "    elif node_name == \"executor\":\n",
    "        print(\"3. EXECUTOR reads plan from context + original query ‚Üí produces result ‚Üí stores in context\")\n",
    "        # Executor reads plan from context\n",
    "        plan = cumulative_state[\"context\"].get(\"last_plan\", \"\")\n",
    "        print(f\"üì• INPUT:  context['last_plan'] = '{plan[:70]}...'\")\n",
    "        print(f\"           messages[0] = '{test_query}'\")\n",
    "\n",
    "        \n",
    "    elif node_name == \"evaluator\":\n",
    "        print(\"   4. EVALUATOR reads result from context ‚Üí produces evaluation ‚Üí routes to end\")\n",
    "        # Evaluator reads result from context\n",
    "        result = cumulative_state[\"context\"].get(\"last_result\", \"\")\n",
    "        print(f\"üì• INPUT:  context['last_result'] = '{result[:70]}...'\")\n",
    "\n",
    "    # Show what this node PRODUCES\n",
    "    if \"messages\" in node_output and node_output[\"messages\"]:\n",
    "        msg = node_output[\"messages\"][-1].content\n",
    "        print(f\"üì§ OUTPUT: '{msg[:70]}...'\")\n",
    "    \n",
    "    # Show what this node STORES for next node\n",
    "    if \"context\" in node_output and node_output[\"context\"]:\n",
    "        for key, value in node_output[\"context\"].items():\n",
    "            print(f\"üíæ STORES: context['{key}'] = '{str(value)[:70]}...'\")\n",
    "    \n",
    "    # Show routing\n",
    "    if \"next_action\" in node_output:\n",
    "        next_step = node_output[\"next_action\"]\n",
    "        print(f\"üîÄ ROUTES: ‚Üí {next_step}\")\n",
    "    \n",
    "    print()  # Blank line for readability\n",
    "    \n",
    "    # Update cumulative state for next node\n",
    "    if \"messages\" in node_output:\n",
    "        cumulative_state[\"messages\"].extend(node_output[\"messages\"])\n",
    "    if \"context\" in node_output:\n",
    "        cumulative_state[\"context\"].update(node_output[\"context\"])\n",
    "    if \"next_action\" in node_output:\n",
    "        cumulative_state[\"next_action\"] = node_output[\"next_action\"]\n",
    "    if \"iterations\" in node_output:\n",
    "        cumulative_state[\"iterations\"] = cumulative_state.get(\"iterations\", 0) + node_output[\"iterations\"]\n",
    "    if \"error_count\" in node_output:\n",
    "        cumulative_state[\"error_count\"] = cumulative_state.get(\"error_count\", 0) + node_output[\"error_count\"]\n",
    "\n",
    "\n",
    "# Final summary\n",
    "print(f\"{'='*80}\")\n",
    "print(\"‚úÖ COMMUNICATION FLOW VERIFIED\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(\"üîó Summary:\")\n",
    "print(\"   1. ANALYZER reads user query ‚Üí produces analysis ‚Üí stores in context\")\n",
    "print(\"   2. PLANNER reads analysis from messages[-3:] ‚Üí produces plan ‚Üí stores in context\")\n",
    "print(\"   3. EXECUTOR reads plan from context + original query ‚Üí produces result ‚Üí stores in context\")\n",
    "print(\"   4. EVALUATOR reads result from context ‚Üí produces evaluation ‚Üí routes to end\")\n",
    "print(\"\\n‚ú® Each node explicitly uses the previous node's output!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ How Nodes Communicate: Code Analysis (with RAG)\n",
    "\n",
    "### 1Ô∏è‚É£ **Analyzer ‚Üí Planner** (via messages list)\n",
    "\n",
    "**Analyzer OUTPUT:**\n",
    "```python\n",
    "# In _analyze_query():\n",
    "return {\n",
    "    \"messages\": [AIMessage(content=f\"Analysis: {response}\")],\n",
    "    \"context\": {\"last_analysis\": response}  # ‚Üê Stored but not used by planner\n",
    "}\n",
    "```\n",
    "\n",
    "**Planner INPUT:**\n",
    "```python\n",
    "# In _plan_response():\n",
    "messages = state[\"messages\"]  # ‚Üê Gets ALL accumulated messages\n",
    "context = \"\\n\".join([str(m.content) for m in messages[-3:]])  # ‚Üê Reads last 3 (includes analyzer's output)\n",
    "```\n",
    "\n",
    "**What's passed**: Analyzer's analysis message  \n",
    "**How**: Via `messages` list (planner reads `messages[-3:]`)\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ **Planner ‚Üí Executor** (via context dict)\n",
    "\n",
    "**Planner OUTPUT:**\n",
    "```python\n",
    "# In _plan_response():\n",
    "return {\n",
    "    \"messages\": [AIMessage(content=f\"Plan: {response}\")],\n",
    "    \"context\": {\"last_plan\": response}  # ‚Üê Stores plan for executor\n",
    "}\n",
    "```\n",
    "\n",
    "**Executor INPUT:**\n",
    "```python\n",
    "# In _execute_plan():\n",
    "plan = state.get(\"context\", {}).get(\"last_plan\", \"\")  # ‚Üê Reads planner's plan\n",
    "original_query = messages[0].content  # ‚Üê Also reads original query\n",
    "```\n",
    "\n",
    "**What's passed**: Planner's 2-3 step action plan + original query  \n",
    "**How**: Via `state[\"context\"][\"last_plan\"]` (structured storage)\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ **Executor ‚Üí Evaluator** (via context dict) **[RAG ENABLED]**\n",
    "\n",
    "**Executor OUTPUT (with RAG):**\n",
    "```python\n",
    "# In _execute_plan():\n",
    "# Step 1: Retrieve relevant documents\n",
    "if self.retriever:\n",
    "    retrieved_docs = self.retriever.invoke(original_query)  # ‚Üê RAG retrieval\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# Step 2: Synthesize with plan and retrieved context\n",
    "prompt = f\"\"\"Original Query: {original_query}\n",
    "Plan: {plan}\n",
    "Retrieved Context: {context_text}  # ‚Üê Grounded in knowledge base\n",
    "Task: Synthesize...\"\"\"\n",
    "\n",
    "response = self._handle_llm_call(prompt, system_prompt)\n",
    "\n",
    "return {\n",
    "    \"messages\": [AIMessage(content=f\"Result: {response}\")],\n",
    "    \"context\": {\"last_result\": response}  # ‚Üê Stores result for evaluator\n",
    "}\n",
    "```\n",
    "\n",
    "**Evaluator INPUT:**\n",
    "```python\n",
    "# In _evaluate_result():\n",
    "result = state.get(\"context\", {}).get(\"last_result\", \"\")  # ‚Üê Reads executor's result\n",
    "```\n",
    "\n",
    "**What's passed**: Executor's comprehensive answer (grounded in retrieved documents when RAG is enabled)  \n",
    "**How**: Via `state[\"context\"][\"last_result\"]` (structured storage)\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ **Evaluator ‚Üí Analyzer** (loop back if needs improvement)\n",
    "\n",
    "**Evaluator OUTPUT:**\n",
    "```python\n",
    "# In _evaluate_result():\n",
    "return {\n",
    "    \"messages\": [AIMessage(content=f\"Evaluation: {response}\")],\n",
    "    \"next_action\": \"continue\" if needs_improvement else \"end\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Routing Decision:**\n",
    "```python\n",
    "# In _should_continue():\n",
    "if next_action == \"continue\":\n",
    "    return \"continue\"  # ‚Üê Routes back to analyzer\n",
    "```\n",
    "\n",
    "**What's passed**: Entire conversation history (all previous messages + context)  \n",
    "**How**: Full state preserved via LangGraph's state management\n",
    "\n",
    "---\n",
    "\n",
    "### üîë Three Communication Patterns:\n",
    "\n",
    "1. **Messages List** (`messages[-3:]`): Used when node needs conversation context\n",
    "   - Planner uses this to read Analyzer's output\n",
    "   \n",
    "2. **Context Dict** (`context['key']`): Used for specific structured data\n",
    "   - Executor reads `last_plan` from Planner\n",
    "   - Evaluator reads `last_result` from Executor\n",
    "\n",
    "3. **Original Query** (`messages[0]`): Always accessible to all nodes\n",
    "   - **Executor uses it for RAG retrieval** via `retriever.invoke(original_query)`\n",
    "\n",
    "4. **RAG Retrieval** (when retriever is provided):\n",
    "   - Executor retrieves documents from vector store\n",
    "   - Synthesizes retrieved context with plan\n",
    "   - Produces grounded responses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Visual Data Flow Diagram\n",
    "\n",
    "```\n",
    "USER QUERY: \"What is the attention mechanism in transformers?\"\n",
    "      |\n",
    "      v\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  ANALYZER NODE                                   ‚îÇ\n",
    "‚îÇ  INPUT:  Original query                         ‚îÇ\n",
    "‚îÇ  OUTPUT: \"Analysis: User wants to know about    ‚îÇ\n",
    "‚îÇ          attention mechanism...\"                 ‚îÇ\n",
    "‚îÇ  STORES: context['last_analysis'] = \"...\"       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                      ‚îÇ\n",
    "                      v\n",
    "            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "            ‚îÇ   STATE PASSED:     ‚îÇ\n",
    "            ‚îÇ - messages: [       ‚îÇ\n",
    "            ‚îÇ     HumanMessage,   ‚îÇ\n",
    "            ‚îÇ     AIMessage]      ‚îÇ\n",
    "            ‚îÇ - context: {        ‚îÇ\n",
    "            ‚îÇ     last_analysis   ‚îÇ\n",
    "            ‚îÇ   }                 ‚îÇ\n",
    "            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                      ‚îÇ\n",
    "                      v\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  PLANNER NODE                                    ‚îÇ\n",
    "‚îÇ  INPUT:  messages[-3:] ‚Üê Reads analyzer output  ‚îÇ\n",
    "‚îÇ  OUTPUT: \"Plan: 1. Define attention 2. Explain  ‚îÇ\n",
    "‚îÇ          mechanism 3. Provide examples...\"       ‚îÇ\n",
    "‚îÇ  STORES: context['last_plan'] = \"...\"           ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                      ‚îÇ\n",
    "                      v\n",
    "            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "            ‚îÇ   STATE PASSED:     ‚îÇ\n",
    "            ‚îÇ - messages: [       ‚îÇ\n",
    "            ‚îÇ     HumanMessage,   ‚îÇ\n",
    "            ‚îÇ     AIMessage,      ‚îÇ\n",
    "            ‚îÇ     AIMessage]      ‚îÇ\n",
    "            ‚îÇ - context: {        ‚îÇ\n",
    "            ‚îÇ     last_plan       ‚îÇ\n",
    "            ‚îÇ   }                 ‚îÇ\n",
    "            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                      ‚îÇ\n",
    "                      v\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  EXECUTOR NODE (with RAG)                        ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ 1. RETRIEVAL STEP                         ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ    retriever.invoke(original_query)       ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ           ‚Üì                                ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ    Vector Store (Chroma) with MMR         ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ           ‚Üì                                ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ    Top-3 Relevant Documents               ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ  ‚îÇ 2. SYNTHESIS STEP                         ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ    INPUT:  context['last_plan']           ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ            messages[0] (original query)   ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ            retrieved_docs (from vector DB)‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ    OUTPUT: \"Result: Attention mechanism   ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ            allows models to focus on...   ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îÇ            [grounded in retrieved docs]\"  ‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îÇ  STORES: context['last_result'] = \"...\"         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                      ‚îÇ\n",
    "                      v\n",
    "            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "            ‚îÇ   STATE PASSED:     ‚îÇ\n",
    "            ‚îÇ - messages: [       ‚îÇ\n",
    "            ‚îÇ     HumanMessage,   ‚îÇ\n",
    "            ‚îÇ     AIMessage,      ‚îÇ\n",
    "            ‚îÇ     AIMessage,      ‚îÇ\n",
    "            ‚îÇ     AIMessage]      ‚îÇ\n",
    "            ‚îÇ - context: {        ‚îÇ\n",
    "            ‚îÇ     last_result     ‚îÇ\n",
    "            ‚îÇ   }                 ‚îÇ\n",
    "            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                      ‚îÇ\n",
    "                      v\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  EVALUATOR NODE                                  ‚îÇ\n",
    "‚îÇ  INPUT:  context['last_result'] ‚Üê Reads result  ‚îÇ\n",
    "‚îÇ  OUTPUT: \"Evaluation: SATISFACTORY\"              ‚îÇ\n",
    "‚îÇ  ROUTES: \"end\" (or \"continue\" if needs work)    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                      ‚îÇ\n",
    "                      v\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ   END NODE    ‚îÇ\n",
    "              ‚îÇ   (Complete)  ‚îÇ\n",
    "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "‚úÖ **Each node reads SPECIFIC data from previous nodes**  \n",
    "‚úÖ **Executor performs RAG: retrieval ‚Üí synthesis with plan**  \n",
    "‚úÖ **Context dict grows with each node** (last_analysis ‚Üí last_plan ‚Üí last_result)  \n",
    "‚úÖ **Messages list accumulates ALL outputs** (for conversation history)  \n",
    "‚úÖ **State is MERGED at each step** (not replaced)  \n",
    "‚úÖ **RAG enables grounded responses** from knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Workflow Architecture (with RAG)\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    User Query Input                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                        ‚îÇ\n",
    "                        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  ANALYZER NODE                                               ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Identifies query intent                                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Extracts key requirements                                 ‚îÇ\n",
    "‚îÇ  ‚Ä¢ System Prompt: \"Expert analyst\"                          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                        ‚îÇ\n",
    "                        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  PLANNER NODE                                                ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Creates 2-3 step action plan                             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Considers context from analyzer                           ‚îÇ\n",
    "‚îÇ  ‚Ä¢ System Prompt: \"Strategic planner\"                       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                        ‚îÇ\n",
    "                        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  EXECUTOR NODE (with RAG)                                    ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n",
    "‚îÇ  ‚îÇ  RAG STEP 1: Document Retrieval                     ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚Ä¢ Query: Original user question                    ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚Ä¢ Retriever: MMR search (k=3, fetch_k=10)          ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚Ä¢ Source: Chroma vector store                      ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚Ä¢ Result: Top-3 relevant documents                 ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n",
    "‚îÇ  ‚îÇ  RAG STEP 2: Synthesis & Generation                 ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚Ä¢ Combines: Plan + Retrieved Docs + Original Query ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚Ä¢ Generates: Grounded, comprehensive answer        ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ  ‚Ä¢ System Prompt: \"Knowledgeable assistant\"         ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                        ‚îÇ\n",
    "                        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  EVALUATOR NODE                                              ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Assesses response quality                                 ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Decides: SATISFACTORY or NEEDS_IMPROVEMENT               ‚îÇ\n",
    "‚îÇ  ‚Ä¢ System Prompt: \"Quality assessor\"                        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                        ‚îÇ\n",
    "                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                ‚îÇ               ‚îÇ\n",
    "                ‚ñº               ‚ñº\n",
    "        SATISFACTORY      NEEDS_IMPROVEMENT\n",
    "        (max iterations)  (error threshold)\n",
    "                ‚îÇ               ‚îÇ\n",
    "                ‚îÇ               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                ‚îÇ                          ‚îÇ\n",
    "                ‚ñº                          ‚ñº\n",
    "            END NODE              Back to ANALYZER\n",
    "                                  (Iteration Loop)\n",
    "```\n",
    "\n",
    "### Key Features at Each Layer\n",
    "\n",
    "**üîÑ Retry Policies**: Each node auto-retries up to 3 times on failure  \n",
    "**üíæ Checkpointing**: State persisted after each node (thread-based)  \n",
    "**üöÄ Caching**: System prompts cached (128 max) for performance  \n",
    "**üìä State Management**: Reducer functions handle message accumulation  \n",
    "**üìö RAG Integration**: Executor retrieves & synthesizes knowledge base documents  \n",
    "**üîç Vector Store**: Chroma with HuggingFace embeddings (all-MiniLM-L6-v2)  \n",
    "**üéØ MMR Search**: Maximum Marginal Relevance for diverse results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Implementation Summary\n",
    "\n",
    "### Benefits Achieved:\n",
    "\n",
    "‚úÖ **OpenAI Compatibility**: Uses OpenAI-compatible message format  \n",
    "‚úÖ **Simpler Message Format**: No custom message classes needed  \n",
    "‚úÖ **Multi-Provider Support**: Access to 14+ inference providers  \n",
    "‚úÖ **Maintained Compatibility**: All retry policies, caching, and error handling intact  \n",
    "‚úÖ **Production Ready**: Tested and verified with complex multi-step workflows  \n",
    "‚úÖ **Streaming Support**: Native streaming capabilities available  \n",
    "‚úÖ **RAG Integration**: Retrieval-Augmented Generation with LangChain vector stores  \n",
    "‚úÖ **Grounded Responses**: Answers backed by knowledge base documents  \n",
    "‚úÖ **Reduced Hallucination**: Facts from vector store, not pure generation  \n",
    "‚úÖ **Flexible Architecture**: Works with or without retriever\n",
    "\n",
    "### RAG Components:\n",
    "\n",
    "üîç **Vector Store**: Chroma with persistent storage  \n",
    "üß† **Embeddings**: HuggingFace all-MiniLM-L6-v2  \n",
    "üéØ **Search Strategy**: MMR (Maximum Marginal Relevance)  \n",
    "üìä **Retrieval**: Top-3 documents with diversity balancing  \n",
    "üîó **Integration**: Seamless synthesis in Executor node\n",
    "\n",
    "### Next Steps (Optional):\n",
    "\n",
    "1. **Provider Selection**: Specify providers (hyperbolic, nebius, together) for better performance\n",
    "2. **Performance Tuning**: Optimize temperature and max_tokens for specific use cases\n",
    "3. **Multi-Model Support**: Add ability to switch between different HuggingFace models\n",
    "4. **Enhanced Caching**: Implement persistent caching for production deployment\n",
    "5. **RAG Optimization**: Tune retriever parameters (k, fetch_k, lambda_mult) for your domain\n",
    "6. **Custom Embeddings**: Experiment with domain-specific embedding models\n",
    "7. **Hybrid Search**: Combine semantic and keyword search for better retrieval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
